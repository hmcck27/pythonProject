{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kobart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WruTEsctOmB",
        "outputId": "fc02adcb-b059-4bed-f799-78c55567034c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 14 00:54:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mvPLavSfdaW",
        "outputId": "4e7bd2df-adc2-4b7c-d508-14f515e706e2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nBX57Elf5vd"
      },
      "source": [
        "#!git clone https://github.com/seujung/KoBART-summarization /content/drive/MyDrive/kobart\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx4IasSmkUEa",
        "outputId": "7cce6d32-6f7d-4180-f4b2-bfc8de85bdc7"
      },
      "source": [
        "cd /content/drive/MyDrive/kobart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/kobart\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "49sa_SEVvRs-",
        "outputId": "3e1b9dc1-4963-4e93-820c-98588e9f886c"
      },
      "source": [
        "\"\"\"\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/kobart/data/test.tsv\", sep = '\\t')\n",
        "df.columns\n",
        "df.head(1)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndf = pd.read_csv(\"/content/drive/MyDrive/kobart/data/test.tsv\", sep = \\'\\t\\')\\ndf.columns\\ndf.head(1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KDu_wCKx5i8",
        "outputId": "f1cd204a-574b-485a-ac93-9037f1e2ca12"
      },
      "source": [
        "!pip install gdown\n",
        "!python download_binary.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "== Data existed.==\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bSEaO9zqgBnr",
        "outputId": "0a7048dc-1d64-41c3-a8a6-216be2b38c4e"
      },
      "source": [
        "#!cd KoBART-summarization/\n",
        "!pip install -r requirements.txt\n",
        "!pip install torchtext==0.8.0 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Collecting torch==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 20kB/s \n",
            "\u001b[?25hCollecting transformers==4.3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 50.0MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d0/84a2f072cd407f93a1e50dff059656bce305f084e63a45cbbceb2fdb67b4/pytorch_lightning-1.1.0-py3-none-any.whl (665kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 42.9MB/s \n",
            "\u001b[?25hCollecting streamlit==0.72.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/3b/8b70128553de980a5120b512c8eedc3667deced9554fc399703414b1d8cf/streamlit-0.72.0-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (3.0.12)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 41.6MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.1MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 60.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (2.4.1)\n",
            "Collecting watchdog\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f6/6b538562aaa62294ca0a1d18b59d9fcb1a43fe166fa5b3a258f445d64119/watchdog-2.1.1-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (1.5.1)\n",
            "Collecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 61.7MB/s \n",
            "\u001b[?25hCollecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (4.1.0)\n",
            "Collecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (0.10.2)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (4.2.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.3->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (1.30.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (2.6.0)\n",
            "Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6d/6c8fe4b658f77947d4244ce81f60230c4c8d1dc1a21ae83e63b269339178/ipykernel-5.5.5-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.0.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (7.6.3)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit==0.72.0->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (0.2.8)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (1.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.3.5)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.0->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (22.0.3)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.7.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.0.18)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.9.4)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.5.1)\n",
            "Building wheels for collected packages: future, blinker\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=5bc38281825819974b19e2622645890d968855726ef71f274b0d5d13f7d82e91\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=60db82bc6bd7a35755b3b9dbf1156b9c54b696bc395bef29b3ef81fa99349997\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built future blinker\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, sacremoses, tokenizers, transformers, future, PyYAML, fsspec, pytorch-lightning, watchdog, blinker, smmap, gitdb, gitpython, base58, ipykernel, pydeck, validators, streamlit\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed PyYAML-5.4.1 base58-2.1.0 blinker-1.4 fsspec-2021.4.0 future-0.18.2 gitdb-4.0.7 gitpython-3.1.14 ipykernel-5.5.5 pydeck-0.6.2 pytorch-lightning-1.1.0 sacremoses-0.0.45 smmap-4.0.0 streamlit-0.72.0 tokenizers-0.10.2 torch-1.7.1 transformers-4.3.3 validators-0.18.2 watchdog-2.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/8a/e09b9b82d4dd676f17aa681003a7533765346744391966dec0d5dba03ee4/torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed torchtext-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRu8_497oLEy",
        "outputId": "e95b6b78-50f4-439e-8109-3f4d8789406d"
      },
      "source": [
        "!bash install_kobart.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kobart\n",
            "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-ofgbtxf2/kobart\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoBART /tmp/pip-install-ofgbtxf2/kobart\n",
            "Requirement already satisfied: transformers==4.3.3 in /usr/local/lib/python3.7/dist-packages (from kobart) (4.3.3)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from kobart) (1.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.3->kobart) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->kobart) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (7.1.2)\n",
            "Building wheels for collected packages: kobart\n",
            "  Building wheel for kobart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobart: filename=kobart-0.4-cp37-none-any.whl size=8502 sha256=4cde3dd220ab49c7688966b9009b41e3471bf512258a348e707ea190bc7eef10\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x8g713mn/wheels/4a/90/60/b0692d915b1dcfa5556fcca48a1422e031118bbae70977d2ae\n",
            "Successfully built kobart\n",
            "Installing collected packages: kobart\n",
            "Successfully installed kobart-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqLb2tA-CM4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPC6X8Caj-9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46c55d0-3637-49be-ea36-08d75f651179"
      },
      "source": [
        "!python train.py  --gradient_clip_val 1.0 --max_epochs 40 --default_root_dir logs  --gpus 1 --batch_size 4 --num_workers 4 --resume_from_checkpoint '/content/drive/MyDrive/kobart/logs/kobart_summary-last.ckpt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=4, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='logs', deterministic=False, distributed_backend=None, enable_pl_optimizer=True, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=40, max_len=512, max_steps=None, min_epochs=1, min_steps=None, model_path=None, move_metrics_to_cpu=False, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint='/content/drive/MyDrive/kobart/logs/kobart_summary-last.ckpt', sync_batchnorm=False, terminate_on_nan=False, test_file='data/test.tsv', tpu_cores=<function _gpus_arg_default at 0x7f02e475a560>, track_grad_norm=-1, train_file='data/train.tsv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate < 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate >= 20 in Trainer.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "INFO:lightning:GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:root:number of workers 4, data length 34242\n",
            "INFO:root:num_train_steps : 85605\n",
            "INFO:root:num_warmup_steps : 8560\n",
            "2021-05-14 01:04:35.723150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "INFO:lightning:\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:234: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "Epoch 37:  80% 8561/10702 [33:44<08:26,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 37:  80% 8563/10702 [33:44<08:25,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8565/10702 [33:44<08:25,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8567/10702 [33:44<08:24,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8569/10702 [33:45<08:24,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8571/10702 [33:45<08:23,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8573/10702 [33:45<08:22,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8575/10702 [33:45<08:22,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8577/10702 [33:45<08:21,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8579/10702 [33:45<08:21,  4.23it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8581/10702 [33:45<08:20,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8583/10702 [33:46<08:20,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8585/10702 [33:46<08:19,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8587/10702 [33:46<08:19,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8589/10702 [33:46<08:18,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8591/10702 [33:46<08:17,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8593/10702 [33:46<08:17,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8595/10702 [33:46<08:16,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8597/10702 [33:47<08:16,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8599/10702 [33:47<08:15,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8601/10702 [33:47<08:15,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8603/10702 [33:47<08:14,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8605/10702 [33:47<08:14,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8607/10702 [33:47<08:13,  4.24it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8609/10702 [33:47<08:12,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8611/10702 [33:47<08:12,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8613/10702 [33:48<08:11,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  80% 8615/10702 [33:48<08:11,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8617/10702 [33:48<08:10,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8619/10702 [33:48<08:10,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8621/10702 [33:48<08:09,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8623/10702 [33:48<08:09,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8625/10702 [33:48<08:08,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8627/10702 [33:49<08:08,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8629/10702 [33:49<08:07,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8631/10702 [33:49<08:06,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8633/10702 [33:49<08:06,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8635/10702 [33:49<08:05,  4.25it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8637/10702 [33:49<08:05,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8639/10702 [33:49<08:04,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8641/10702 [33:49<08:04,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8643/10702 [33:50<08:03,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8645/10702 [33:50<08:03,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8647/10702 [33:50<08:02,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8649/10702 [33:50<08:01,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8651/10702 [33:50<08:01,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8653/10702 [33:50<08:00,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8655/10702 [33:50<08:00,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8657/10702 [33:51<07:59,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8659/10702 [33:51<07:59,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8661/10702 [33:51<07:58,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8663/10702 [33:51<07:58,  4.26it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8665/10702 [33:51<07:57,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8667/10702 [33:51<07:57,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8669/10702 [33:51<07:56,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8671/10702 [33:52<07:55,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8673/10702 [33:52<07:55,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8675/10702 [33:52<07:54,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8677/10702 [33:52<07:54,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8679/10702 [33:52<07:53,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8681/10702 [33:52<07:53,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8683/10702 [33:52<07:52,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8685/10702 [33:52<07:52,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8687/10702 [33:53<07:51,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8689/10702 [33:53<07:51,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8691/10702 [33:53<07:50,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8693/10702 [33:53<07:49,  4.27it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8695/10702 [33:53<07:49,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8697/10702 [33:53<07:48,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8699/10702 [33:53<07:48,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8701/10702 [33:54<07:47,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8703/10702 [33:54<07:47,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8705/10702 [33:54<07:46,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8707/10702 [33:54<07:46,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8709/10702 [33:54<07:45,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8711/10702 [33:54<07:45,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8713/10702 [33:54<07:44,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8715/10702 [33:55<07:43,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8717/10702 [33:55<07:43,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8719/10702 [33:55<07:42,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  81% 8721/10702 [33:55<07:42,  4.28it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8723/10702 [33:55<07:41,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8725/10702 [33:55<07:41,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8727/10702 [33:55<07:40,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8729/10702 [33:55<07:40,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8731/10702 [33:56<07:39,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8733/10702 [33:56<07:39,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8735/10702 [33:56<07:38,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8737/10702 [33:56<07:38,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8739/10702 [33:56<07:37,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8741/10702 [33:56<07:36,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8743/10702 [33:56<07:36,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8745/10702 [33:57<07:35,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8747/10702 [33:57<07:35,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8749/10702 [33:57<07:34,  4.29it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8751/10702 [33:57<07:34,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8753/10702 [33:57<07:33,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8755/10702 [33:57<07:33,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8757/10702 [33:57<07:32,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8759/10702 [33:58<07:32,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8761/10702 [33:58<07:31,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8763/10702 [33:58<07:31,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8765/10702 [33:58<07:30,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8767/10702 [33:58<07:29,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8769/10702 [33:58<07:29,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8771/10702 [33:58<07:28,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8773/10702 [33:58<07:28,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8775/10702 [33:59<07:27,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8777/10702 [33:59<07:27,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8779/10702 [33:59<07:26,  4.30it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8781/10702 [33:59<07:26,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8783/10702 [33:59<07:25,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8785/10702 [33:59<07:25,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8787/10702 [33:59<07:24,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8789/10702 [34:00<07:24,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8791/10702 [34:00<07:23,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8793/10702 [34:00<07:22,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8795/10702 [34:00<07:22,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8797/10702 [34:00<07:21,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8799/10702 [34:00<07:21,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8801/10702 [34:00<07:20,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8803/10702 [34:00<07:20,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8805/10702 [34:01<07:19,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8807/10702 [34:01<07:19,  4.31it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8809/10702 [34:01<07:18,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8811/10702 [34:01<07:18,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8813/10702 [34:01<07:17,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8815/10702 [34:01<07:17,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8817/10702 [34:01<07:16,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8819/10702 [34:02<07:16,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8821/10702 [34:02<07:15,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8823/10702 [34:02<07:14,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8825/10702 [34:02<07:14,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8827/10702 [34:02<07:13,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  82% 8829/10702 [34:02<07:13,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8831/10702 [34:02<07:12,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8833/10702 [34:03<07:12,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8835/10702 [34:03<07:11,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8837/10702 [34:03<07:11,  4.32it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8839/10702 [34:03<07:10,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8841/10702 [34:03<07:10,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8843/10702 [34:03<07:09,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8845/10702 [34:03<07:09,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8847/10702 [34:03<07:08,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8849/10702 [34:04<07:08,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8851/10702 [34:04<07:07,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8853/10702 [34:04<07:06,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8855/10702 [34:04<07:06,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8857/10702 [34:04<07:05,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8859/10702 [34:04<07:05,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8861/10702 [34:04<07:04,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8863/10702 [34:05<07:04,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8865/10702 [34:05<07:03,  4.33it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8867/10702 [34:05<07:03,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8869/10702 [34:05<07:02,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8871/10702 [34:05<07:02,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8873/10702 [34:05<07:01,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8875/10702 [34:05<07:01,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8877/10702 [34:06<07:00,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8879/10702 [34:06<07:00,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8881/10702 [34:06<06:59,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8883/10702 [34:06<06:59,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8885/10702 [34:06<06:58,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8887/10702 [34:06<06:57,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8889/10702 [34:06<06:57,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8891/10702 [34:06<06:56,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8893/10702 [34:07<06:56,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8895/10702 [34:07<06:55,  4.34it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8897/10702 [34:07<06:55,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8899/10702 [34:07<06:54,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8901/10702 [34:07<06:54,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8903/10702 [34:07<06:53,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8905/10702 [34:07<06:53,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8907/10702 [34:08<06:52,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8909/10702 [34:08<06:52,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8911/10702 [34:08<06:51,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8913/10702 [34:08<06:51,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8915/10702 [34:08<06:50,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8917/10702 [34:08<06:50,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8919/10702 [34:08<06:49,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8921/10702 [34:09<06:49,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8923/10702 [34:09<06:48,  4.35it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8925/10702 [34:09<06:48,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8927/10702 [34:09<06:47,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8929/10702 [34:09<06:46,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8931/10702 [34:09<06:46,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8933/10702 [34:09<06:45,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  83% 8935/10702 [34:09<06:45,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8937/10702 [34:10<06:44,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8939/10702 [34:10<06:44,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8941/10702 [34:10<06:43,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8943/10702 [34:10<06:43,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8945/10702 [34:10<06:42,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8947/10702 [34:10<06:42,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8949/10702 [34:10<06:41,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8951/10702 [34:11<06:41,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8953/10702 [34:11<06:40,  4.36it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8955/10702 [34:11<06:40,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8957/10702 [34:11<06:39,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8959/10702 [34:11<06:39,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8961/10702 [34:11<06:38,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8963/10702 [34:11<06:38,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8965/10702 [34:11<06:37,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8967/10702 [34:12<06:37,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8969/10702 [34:12<06:36,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8971/10702 [34:12<06:36,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8973/10702 [34:12<06:35,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8975/10702 [34:12<06:34,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8977/10702 [34:12<06:34,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8979/10702 [34:12<06:33,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8981/10702 [34:13<06:33,  4.37it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8983/10702 [34:13<06:32,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8985/10702 [34:13<06:32,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8987/10702 [34:13<06:31,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8989/10702 [34:13<06:31,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8991/10702 [34:13<06:30,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8993/10702 [34:13<06:30,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8995/10702 [34:14<06:29,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8997/10702 [34:14<06:29,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 8999/10702 [34:14<06:28,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9001/10702 [34:14<06:28,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9003/10702 [34:14<06:27,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9005/10702 [34:14<06:27,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9007/10702 [34:14<06:26,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9009/10702 [34:14<06:26,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9011/10702 [34:15<06:25,  4.38it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9013/10702 [34:15<06:25,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9015/10702 [34:15<06:24,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9017/10702 [34:15<06:24,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9019/10702 [34:15<06:23,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9021/10702 [34:15<06:23,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9023/10702 [34:15<06:22,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9025/10702 [34:16<06:22,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9027/10702 [34:16<06:21,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9029/10702 [34:16<06:21,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9031/10702 [34:16<06:20,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9033/10702 [34:16<06:19,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9035/10702 [34:16<06:19,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9037/10702 [34:16<06:18,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9039/10702 [34:17<06:18,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9041/10702 [34:17<06:17,  4.39it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  84% 9043/10702 [34:17<06:17,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9045/10702 [34:17<06:16,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9047/10702 [34:17<06:16,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9049/10702 [34:17<06:15,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9051/10702 [34:17<06:15,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9053/10702 [34:17<06:14,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9055/10702 [34:18<06:14,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9057/10702 [34:18<06:13,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9059/10702 [34:18<06:13,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9061/10702 [34:18<06:12,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9063/10702 [34:18<06:12,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9065/10702 [34:18<06:11,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9067/10702 [34:18<06:11,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9069/10702 [34:19<06:10,  4.40it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9071/10702 [34:19<06:10,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9073/10702 [34:19<06:09,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9075/10702 [34:19<06:09,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9077/10702 [34:19<06:08,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9079/10702 [34:19<06:08,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9081/10702 [34:19<06:07,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9083/10702 [34:20<06:07,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9085/10702 [34:20<06:06,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9087/10702 [34:20<06:06,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9089/10702 [34:20<06:05,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9091/10702 [34:20<06:05,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9093/10702 [34:20<06:04,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9095/10702 [34:20<06:04,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9097/10702 [34:20<06:03,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9099/10702 [34:21<06:03,  4.41it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9101/10702 [34:21<06:02,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9103/10702 [34:21<06:02,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9105/10702 [34:21<06:01,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9107/10702 [34:21<06:01,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9109/10702 [34:21<06:00,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9111/10702 [34:21<06:00,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9113/10702 [34:22<05:59,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9115/10702 [34:22<05:59,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9117/10702 [34:22<05:58,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9119/10702 [34:22<05:58,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9121/10702 [34:22<05:57,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9123/10702 [34:22<05:57,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9125/10702 [34:22<05:56,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9127/10702 [34:23<05:56,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9129/10702 [34:23<05:55,  4.42it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9131/10702 [34:23<05:54,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9133/10702 [34:23<05:54,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9135/10702 [34:23<05:53,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9137/10702 [34:23<05:53,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9139/10702 [34:23<05:52,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9141/10702 [34:23<05:52,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9143/10702 [34:24<05:51,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9145/10702 [34:24<05:51,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9147/10702 [34:24<05:50,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  85% 9149/10702 [34:24<05:50,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9151/10702 [34:24<05:49,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9153/10702 [34:24<05:49,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9155/10702 [34:24<05:48,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9157/10702 [34:25<05:48,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9159/10702 [34:25<05:47,  4.43it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9161/10702 [34:25<05:47,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9163/10702 [34:25<05:46,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9165/10702 [34:25<05:46,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9167/10702 [34:25<05:45,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9169/10702 [34:25<05:45,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9171/10702 [34:26<05:44,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9173/10702 [34:26<05:44,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9175/10702 [34:26<05:43,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9177/10702 [34:26<05:43,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9179/10702 [34:26<05:42,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9181/10702 [34:26<05:42,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9183/10702 [34:26<05:41,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9185/10702 [34:26<05:41,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9187/10702 [34:27<05:40,  4.44it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9189/10702 [34:27<05:40,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9191/10702 [34:27<05:39,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9193/10702 [34:27<05:39,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9195/10702 [34:27<05:38,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9197/10702 [34:27<05:38,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9199/10702 [34:27<05:37,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9201/10702 [34:28<05:37,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9203/10702 [34:28<05:36,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9205/10702 [34:28<05:36,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9207/10702 [34:28<05:35,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9209/10702 [34:28<05:35,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9211/10702 [34:28<05:34,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9213/10702 [34:28<05:34,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9215/10702 [34:29<05:33,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9217/10702 [34:29<05:33,  4.45it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9219/10702 [34:29<05:32,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9221/10702 [34:29<05:32,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9223/10702 [34:29<05:31,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9225/10702 [34:29<05:31,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9227/10702 [34:29<05:30,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9229/10702 [34:29<05:30,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9231/10702 [34:30<05:29,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9233/10702 [34:30<05:29,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9235/10702 [34:30<05:28,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9237/10702 [34:30<05:28,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9239/10702 [34:30<05:27,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9241/10702 [34:30<05:27,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9243/10702 [34:30<05:26,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9245/10702 [34:31<05:26,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9247/10702 [34:31<05:25,  4.46it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9249/10702 [34:31<05:25,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9251/10702 [34:31<05:24,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9253/10702 [34:31<05:24,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9255/10702 [34:31<05:23,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  86% 9257/10702 [34:31<05:23,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9259/10702 [34:32<05:22,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9261/10702 [34:32<05:22,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9263/10702 [34:32<05:21,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9265/10702 [34:32<05:21,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9267/10702 [34:32<05:20,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9269/10702 [34:32<05:20,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9271/10702 [34:32<05:19,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9273/10702 [34:32<05:19,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9275/10702 [34:33<05:18,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9277/10702 [34:33<05:18,  4.47it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9279/10702 [34:33<05:17,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9281/10702 [34:33<05:17,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9283/10702 [34:33<05:16,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9285/10702 [34:33<05:16,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9287/10702 [34:33<05:15,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9289/10702 [34:34<05:15,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9291/10702 [34:34<05:14,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9293/10702 [34:34<05:14,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9295/10702 [34:34<05:14,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9297/10702 [34:34<05:13,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9299/10702 [34:34<05:13,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9301/10702 [34:34<05:12,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9303/10702 [34:34<05:12,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9305/10702 [34:35<05:11,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9307/10702 [34:35<05:11,  4.48it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9309/10702 [34:35<05:10,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9311/10702 [34:35<05:10,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9313/10702 [34:35<05:09,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9315/10702 [34:35<05:09,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9317/10702 [34:35<05:08,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9319/10702 [34:36<05:08,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9321/10702 [34:36<05:07,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9323/10702 [34:36<05:07,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9325/10702 [34:36<05:06,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9327/10702 [34:36<05:06,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9329/10702 [34:36<05:05,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9331/10702 [34:36<05:05,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9333/10702 [34:37<05:04,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9335/10702 [34:37<05:04,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9337/10702 [34:37<05:03,  4.49it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9339/10702 [34:37<05:03,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9341/10702 [34:37<05:02,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9343/10702 [34:37<05:02,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9345/10702 [34:37<05:01,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9347/10702 [34:37<05:01,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9349/10702 [34:38<05:00,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9351/10702 [34:38<05:00,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9353/10702 [34:38<04:59,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9355/10702 [34:38<04:59,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9357/10702 [34:38<04:58,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9359/10702 [34:38<04:58,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9361/10702 [34:38<04:57,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  87% 9363/10702 [34:39<04:57,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9365/10702 [34:39<04:56,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9367/10702 [34:39<04:56,  4.50it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9369/10702 [34:39<04:55,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9371/10702 [34:39<04:55,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9373/10702 [34:39<04:54,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9375/10702 [34:39<04:54,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9377/10702 [34:40<04:53,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9379/10702 [34:40<04:53,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9381/10702 [34:40<04:52,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9383/10702 [34:40<04:52,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9385/10702 [34:40<04:51,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9387/10702 [34:40<04:51,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9389/10702 [34:40<04:50,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9391/10702 [34:40<04:50,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9393/10702 [34:41<04:50,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9395/10702 [34:41<04:49,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9397/10702 [34:41<04:49,  4.51it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9399/10702 [34:41<04:48,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9401/10702 [34:41<04:48,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9403/10702 [34:41<04:47,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9405/10702 [34:41<04:47,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9407/10702 [34:42<04:46,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9409/10702 [34:42<04:46,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9411/10702 [34:42<04:45,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9413/10702 [34:42<04:45,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9415/10702 [34:42<04:44,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9417/10702 [34:42<04:44,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9419/10702 [34:42<04:43,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9421/10702 [34:43<04:43,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9423/10702 [34:43<04:42,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9425/10702 [34:43<04:42,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9427/10702 [34:43<04:41,  4.52it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9429/10702 [34:43<04:41,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9431/10702 [34:43<04:40,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9433/10702 [34:43<04:40,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9435/10702 [34:43<04:39,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9437/10702 [34:44<04:39,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9439/10702 [34:44<04:38,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9441/10702 [34:44<04:38,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9443/10702 [34:44<04:37,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9445/10702 [34:44<04:37,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9447/10702 [34:44<04:36,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9449/10702 [34:44<04:36,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9451/10702 [34:45<04:35,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9453/10702 [34:45<04:35,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9455/10702 [34:45<04:35,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9457/10702 [34:45<04:34,  4.53it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9459/10702 [34:45<04:34,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9461/10702 [34:45<04:33,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9463/10702 [34:45<04:33,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9465/10702 [34:46<04:32,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9467/10702 [34:46<04:32,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9469/10702 [34:46<04:31,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  88% 9471/10702 [34:46<04:31,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9473/10702 [34:46<04:30,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9475/10702 [34:46<04:30,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9477/10702 [34:46<04:29,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9479/10702 [34:46<04:29,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9481/10702 [34:47<04:28,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9483/10702 [34:47<04:28,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9485/10702 [34:47<04:27,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9487/10702 [34:47<04:27,  4.54it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9489/10702 [34:47<04:26,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9491/10702 [34:47<04:26,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9493/10702 [34:47<04:25,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9495/10702 [34:48<04:25,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9497/10702 [34:48<04:24,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9499/10702 [34:48<04:24,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9501/10702 [34:48<04:24,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9503/10702 [34:48<04:23,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9505/10702 [34:48<04:23,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9507/10702 [34:48<04:22,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9509/10702 [34:49<04:22,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9511/10702 [34:49<04:21,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9513/10702 [34:49<04:21,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9515/10702 [34:49<04:20,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9517/10702 [34:49<04:20,  4.55it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9519/10702 [34:49<04:19,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9521/10702 [34:49<04:19,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9523/10702 [34:49<04:18,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9525/10702 [34:50<04:18,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9527/10702 [34:50<04:17,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9529/10702 [34:50<04:17,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9531/10702 [34:50<04:16,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9533/10702 [34:50<04:16,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9535/10702 [34:50<04:15,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9537/10702 [34:50<04:15,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9539/10702 [34:51<04:14,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9541/10702 [34:51<04:14,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9543/10702 [34:51<04:13,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9545/10702 [34:51<04:13,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9547/10702 [34:51<04:13,  4.56it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9549/10702 [34:51<04:12,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9551/10702 [34:51<04:12,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9553/10702 [34:52<04:11,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9555/10702 [34:52<04:11,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9557/10702 [34:52<04:10,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9559/10702 [34:52<04:10,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9561/10702 [34:52<04:09,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9563/10702 [34:52<04:09,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9565/10702 [34:52<04:08,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9567/10702 [34:52<04:08,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9569/10702 [34:53<04:07,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9571/10702 [34:53<04:07,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9573/10702 [34:53<04:06,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9575/10702 [34:53<04:06,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  89% 9577/10702 [34:53<04:05,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9579/10702 [34:53<04:05,  4.57it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9581/10702 [34:53<04:04,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9583/10702 [34:54<04:04,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9585/10702 [34:54<04:04,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9587/10702 [34:54<04:03,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9589/10702 [34:54<04:03,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9591/10702 [34:54<04:02,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9593/10702 [34:54<04:02,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9595/10702 [34:54<04:01,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9597/10702 [34:55<04:01,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9599/10702 [34:55<04:00,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9601/10702 [34:55<04:00,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9603/10702 [34:55<03:59,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9605/10702 [34:55<03:59,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9607/10702 [34:55<03:58,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9609/10702 [34:55<03:58,  4.58it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9611/10702 [34:55<03:57,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9613/10702 [34:56<03:57,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9615/10702 [34:56<03:56,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9617/10702 [34:56<03:56,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9619/10702 [34:56<03:56,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9621/10702 [34:56<03:55,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9623/10702 [34:56<03:55,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9625/10702 [34:56<03:54,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9627/10702 [34:57<03:54,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9629/10702 [34:57<03:53,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9631/10702 [34:57<03:53,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9633/10702 [34:57<03:52,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9635/10702 [34:57<03:52,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9637/10702 [34:57<03:51,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9639/10702 [34:57<03:51,  4.59it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9641/10702 [34:58<03:50,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9643/10702 [34:58<03:50,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9645/10702 [34:58<03:49,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9647/10702 [34:58<03:49,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9649/10702 [34:58<03:49,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9651/10702 [34:58<03:48,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9653/10702 [34:58<03:48,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9655/10702 [34:58<03:47,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9657/10702 [34:59<03:47,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9659/10702 [34:59<03:46,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9661/10702 [34:59<03:46,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9663/10702 [34:59<03:45,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9665/10702 [34:59<03:45,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9667/10702 [34:59<03:44,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9669/10702 [34:59<03:44,  4.60it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9671/10702 [35:00<03:43,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9673/10702 [35:00<03:43,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9675/10702 [35:00<03:42,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9677/10702 [35:00<03:42,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9679/10702 [35:00<03:42,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9681/10702 [35:00<03:41,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9683/10702 [35:00<03:41,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  90% 9685/10702 [35:01<03:40,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9687/10702 [35:01<03:40,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9689/10702 [35:01<03:39,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9691/10702 [35:01<03:39,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9693/10702 [35:01<03:38,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9695/10702 [35:01<03:38,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9697/10702 [35:01<03:37,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9699/10702 [35:02<03:37,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9701/10702 [35:02<03:36,  4.61it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9703/10702 [35:02<03:36,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9705/10702 [35:02<03:35,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9707/10702 [35:02<03:35,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9709/10702 [35:02<03:35,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9711/10702 [35:02<03:34,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9713/10702 [35:02<03:34,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9715/10702 [35:03<03:33,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9717/10702 [35:03<03:33,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9719/10702 [35:03<03:32,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9721/10702 [35:03<03:32,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9723/10702 [35:03<03:31,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9725/10702 [35:03<03:31,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9727/10702 [35:03<03:30,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9729/10702 [35:04<03:30,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9731/10702 [35:04<03:29,  4.62it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9733/10702 [35:04<03:29,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9735/10702 [35:04<03:29,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9737/10702 [35:04<03:28,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9739/10702 [35:04<03:28,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9741/10702 [35:04<03:27,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9743/10702 [35:05<03:27,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9745/10702 [35:05<03:26,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9747/10702 [35:05<03:26,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9749/10702 [35:05<03:25,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9751/10702 [35:05<03:25,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9753/10702 [35:05<03:24,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9755/10702 [35:05<03:24,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9757/10702 [35:05<03:23,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9759/10702 [35:06<03:23,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9761/10702 [35:06<03:23,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9763/10702 [35:06<03:22,  4.63it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9765/10702 [35:06<03:22,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9767/10702 [35:06<03:21,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9769/10702 [35:06<03:21,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9771/10702 [35:06<03:20,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9773/10702 [35:07<03:20,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9775/10702 [35:07<03:19,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9777/10702 [35:07<03:19,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9779/10702 [35:07<03:18,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9781/10702 [35:07<03:18,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9783/10702 [35:07<03:17,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9785/10702 [35:07<03:17,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9787/10702 [35:08<03:17,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9789/10702 [35:08<03:16,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  91% 9791/10702 [35:08<03:16,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9793/10702 [35:08<03:15,  4.64it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9795/10702 [35:08<03:15,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9797/10702 [35:08<03:14,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9799/10702 [35:08<03:14,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9801/10702 [35:08<03:13,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9803/10702 [35:09<03:13,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9805/10702 [35:09<03:12,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9807/10702 [35:09<03:12,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9809/10702 [35:09<03:12,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9811/10702 [35:09<03:11,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9813/10702 [35:09<03:11,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9815/10702 [35:09<03:10,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9817/10702 [35:10<03:10,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9819/10702 [35:10<03:09,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9821/10702 [35:10<03:09,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9823/10702 [35:10<03:08,  4.65it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9825/10702 [35:10<03:08,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9827/10702 [35:10<03:07,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9829/10702 [35:10<03:07,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9831/10702 [35:10<03:07,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9833/10702 [35:11<03:06,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9835/10702 [35:11<03:06,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9837/10702 [35:11<03:05,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9839/10702 [35:11<03:05,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9841/10702 [35:11<03:04,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9843/10702 [35:11<03:04,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9845/10702 [35:11<03:03,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9847/10702 [35:12<03:03,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9849/10702 [35:12<03:02,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9851/10702 [35:12<03:02,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9853/10702 [35:12<03:02,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9855/10702 [35:12<03:01,  4.66it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9857/10702 [35:12<03:01,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9859/10702 [35:12<03:00,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9861/10702 [35:13<03:00,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9863/10702 [35:13<02:59,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9865/10702 [35:13<02:59,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9867/10702 [35:13<02:58,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9869/10702 [35:13<02:58,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9871/10702 [35:13<02:57,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9873/10702 [35:13<02:57,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9875/10702 [35:13<02:57,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9877/10702 [35:14<02:56,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9879/10702 [35:14<02:56,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9881/10702 [35:14<02:55,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9883/10702 [35:14<02:55,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9885/10702 [35:14<02:54,  4.67it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9887/10702 [35:14<02:54,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9889/10702 [35:14<02:53,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9891/10702 [35:15<02:53,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9893/10702 [35:15<02:52,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9895/10702 [35:15<02:52,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9897/10702 [35:15<02:52,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  92% 9899/10702 [35:15<02:51,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9901/10702 [35:15<02:51,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9903/10702 [35:15<02:50,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9905/10702 [35:16<02:50,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9907/10702 [35:16<02:49,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9909/10702 [35:16<02:49,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9911/10702 [35:16<02:48,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9913/10702 [35:16<02:48,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9915/10702 [35:16<02:48,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9917/10702 [35:16<02:47,  4.68it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9919/10702 [35:16<02:47,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9921/10702 [35:17<02:46,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9923/10702 [35:17<02:46,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9925/10702 [35:17<02:45,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9927/10702 [35:17<02:45,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9929/10702 [35:17<02:44,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9931/10702 [35:17<02:44,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9933/10702 [35:17<02:43,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9935/10702 [35:18<02:43,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9937/10702 [35:18<02:43,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9939/10702 [35:18<02:42,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9941/10702 [35:18<02:42,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9943/10702 [35:18<02:41,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9945/10702 [35:18<02:41,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9947/10702 [35:18<02:40,  4.69it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9949/10702 [35:19<02:40,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9951/10702 [35:19<02:39,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9953/10702 [35:19<02:39,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9955/10702 [35:19<02:39,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9957/10702 [35:19<02:38,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9959/10702 [35:19<02:38,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9961/10702 [35:19<02:37,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9963/10702 [35:19<02:37,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9965/10702 [35:20<02:36,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9967/10702 [35:20<02:36,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9969/10702 [35:20<02:35,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9971/10702 [35:20<02:35,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9973/10702 [35:20<02:35,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9975/10702 [35:20<02:34,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9977/10702 [35:20<02:34,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9979/10702 [35:21<02:33,  4.70it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9981/10702 [35:21<02:33,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9983/10702 [35:21<02:32,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9985/10702 [35:21<02:32,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9987/10702 [35:21<02:31,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9989/10702 [35:21<02:31,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9991/10702 [35:21<02:31,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9993/10702 [35:22<02:30,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9995/10702 [35:22<02:30,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9997/10702 [35:22<02:29,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 9999/10702 [35:22<02:29,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 10001/10702 [35:22<02:28,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 10003/10702 [35:22<02:28,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  93% 10005/10702 [35:22<02:27,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10007/10702 [35:22<02:27,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10009/10702 [35:23<02:27,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10011/10702 [35:23<02:26,  4.71it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10013/10702 [35:23<02:26,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10015/10702 [35:23<02:25,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10017/10702 [35:23<02:25,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10019/10702 [35:23<02:24,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10021/10702 [35:23<02:24,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10023/10702 [35:24<02:23,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10025/10702 [35:24<02:23,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10027/10702 [35:24<02:23,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10029/10702 [35:24<02:22,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10031/10702 [35:24<02:22,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10033/10702 [35:24<02:21,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10035/10702 [35:24<02:21,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10037/10702 [35:25<02:20,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10039/10702 [35:25<02:20,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10041/10702 [35:25<02:19,  4.72it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10043/10702 [35:25<02:19,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10045/10702 [35:25<02:19,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10047/10702 [35:25<02:18,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10049/10702 [35:25<02:18,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10051/10702 [35:25<02:17,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10053/10702 [35:26<02:17,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10055/10702 [35:26<02:16,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10057/10702 [35:26<02:16,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10059/10702 [35:26<02:15,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10061/10702 [35:26<02:15,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10063/10702 [35:26<02:15,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10065/10702 [35:26<02:14,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10067/10702 [35:27<02:14,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10069/10702 [35:27<02:13,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10071/10702 [35:27<02:13,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10073/10702 [35:27<02:12,  4.73it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10075/10702 [35:27<02:12,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10077/10702 [35:27<02:11,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10079/10702 [35:27<02:11,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10081/10702 [35:28<02:11,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10083/10702 [35:28<02:10,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10085/10702 [35:28<02:10,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10087/10702 [35:28<02:09,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10089/10702 [35:28<02:09,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10091/10702 [35:28<02:08,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10093/10702 [35:28<02:08,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10095/10702 [35:28<02:08,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10097/10702 [35:29<02:07,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10099/10702 [35:29<02:07,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10101/10702 [35:29<02:06,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10103/10702 [35:29<02:06,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10105/10702 [35:29<02:05,  4.74it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10107/10702 [35:29<02:05,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10109/10702 [35:29<02:04,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10111/10702 [35:30<02:04,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  94% 10113/10702 [35:30<02:04,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10115/10702 [35:30<02:03,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10117/10702 [35:30<02:03,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10119/10702 [35:30<02:02,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10121/10702 [35:30<02:02,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10123/10702 [35:30<02:01,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10125/10702 [35:31<02:01,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10127/10702 [35:31<02:01,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10129/10702 [35:31<02:00,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10131/10702 [35:31<02:00,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10133/10702 [35:31<01:59,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10135/10702 [35:31<01:59,  4.75it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10137/10702 [35:31<01:58,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10139/10702 [35:31<01:58,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10141/10702 [35:32<01:57,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10143/10702 [35:32<01:57,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10145/10702 [35:32<01:57,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10147/10702 [35:32<01:56,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10149/10702 [35:32<01:56,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10151/10702 [35:32<01:55,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10153/10702 [35:32<01:55,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10155/10702 [35:33<01:54,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10157/10702 [35:33<01:54,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10159/10702 [35:33<01:54,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10161/10702 [35:33<01:53,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10163/10702 [35:33<01:53,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10165/10702 [35:33<01:52,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10167/10702 [35:33<01:52,  4.76it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10169/10702 [35:34<01:51,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10171/10702 [35:34<01:51,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10173/10702 [35:34<01:50,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10175/10702 [35:34<01:50,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10177/10702 [35:34<01:50,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10179/10702 [35:34<01:49,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10181/10702 [35:34<01:49,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10183/10702 [35:34<01:48,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10185/10702 [35:35<01:48,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10187/10702 [35:35<01:47,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10189/10702 [35:35<01:47,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10191/10702 [35:35<01:47,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10193/10702 [35:35<01:46,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10195/10702 [35:35<01:46,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10197/10702 [35:35<01:45,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10199/10702 [35:36<01:45,  4.77it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10201/10702 [35:36<01:44,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10203/10702 [35:36<01:44,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10205/10702 [35:36<01:44,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10207/10702 [35:36<01:43,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10209/10702 [35:36<01:43,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10211/10702 [35:36<01:42,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10213/10702 [35:37<01:42,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10215/10702 [35:37<01:41,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10217/10702 [35:37<01:41,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  95% 10219/10702 [35:37<01:41,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10221/10702 [35:37<01:40,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10223/10702 [35:37<01:40,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10225/10702 [35:37<01:39,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10227/10702 [35:37<01:39,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10229/10702 [35:38<01:38,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10231/10702 [35:38<01:38,  4.78it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10233/10702 [35:38<01:38,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10235/10702 [35:38<01:37,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10237/10702 [35:38<01:37,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10239/10702 [35:38<01:36,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10241/10702 [35:38<01:36,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10243/10702 [35:39<01:35,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10245/10702 [35:39<01:35,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10247/10702 [35:39<01:34,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10249/10702 [35:39<01:34,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10251/10702 [35:39<01:34,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10253/10702 [35:39<01:33,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10255/10702 [35:39<01:33,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10257/10702 [35:40<01:32,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10259/10702 [35:40<01:32,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10261/10702 [35:40<01:31,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10263/10702 [35:40<01:31,  4.79it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10265/10702 [35:40<01:31,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10267/10702 [35:40<01:30,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10269/10702 [35:40<01:30,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10271/10702 [35:40<01:29,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10273/10702 [35:41<01:29,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10275/10702 [35:41<01:28,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10277/10702 [35:41<01:28,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10279/10702 [35:41<01:28,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10281/10702 [35:41<01:27,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10283/10702 [35:41<01:27,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10285/10702 [35:41<01:26,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10287/10702 [35:42<01:26,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10289/10702 [35:42<01:25,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10291/10702 [35:42<01:25,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10293/10702 [35:42<01:25,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10295/10702 [35:42<01:24,  4.80it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10297/10702 [35:42<01:24,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10299/10702 [35:42<01:23,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10301/10702 [35:43<01:23,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10303/10702 [35:43<01:22,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10305/10702 [35:43<01:22,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10307/10702 [35:43<01:22,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10309/10702 [35:43<01:21,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10311/10702 [35:43<01:21,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10313/10702 [35:43<01:20,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10315/10702 [35:43<01:20,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10317/10702 [35:44<01:20,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10319/10702 [35:44<01:19,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10321/10702 [35:44<01:19,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10323/10702 [35:44<01:18,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10325/10702 [35:44<01:18,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  96% 10327/10702 [35:44<01:17,  4.81it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10329/10702 [35:44<01:17,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10331/10702 [35:45<01:17,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10333/10702 [35:45<01:16,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10335/10702 [35:45<01:16,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10337/10702 [35:45<01:15,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10339/10702 [35:45<01:15,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10341/10702 [35:45<01:14,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10343/10702 [35:45<01:14,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10345/10702 [35:46<01:14,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10347/10702 [35:46<01:13,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10349/10702 [35:46<01:13,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10351/10702 [35:46<01:12,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10353/10702 [35:46<01:12,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10355/10702 [35:46<01:11,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10357/10702 [35:46<01:11,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10359/10702 [35:46<01:11,  4.82it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10361/10702 [35:47<01:10,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10363/10702 [35:47<01:10,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10365/10702 [35:47<01:09,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10367/10702 [35:47<01:09,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10369/10702 [35:47<01:08,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10371/10702 [35:47<01:08,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10373/10702 [35:47<01:08,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10375/10702 [35:48<01:07,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10377/10702 [35:48<01:07,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10379/10702 [35:48<01:06,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10381/10702 [35:48<01:06,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10383/10702 [35:48<01:06,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10385/10702 [35:48<01:05,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10387/10702 [35:48<01:05,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10389/10702 [35:49<01:04,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10391/10702 [35:49<01:04,  4.83it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10393/10702 [35:49<01:03,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10395/10702 [35:49<01:03,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10397/10702 [35:49<01:03,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10399/10702 [35:49<01:02,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10401/10702 [35:49<01:02,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10403/10702 [35:49<01:01,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10405/10702 [35:50<01:01,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10407/10702 [35:50<01:00,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10409/10702 [35:50<01:00,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10411/10702 [35:50<01:00,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10413/10702 [35:50<00:59,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10415/10702 [35:50<00:59,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10417/10702 [35:50<00:58,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10419/10702 [35:51<00:58,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10421/10702 [35:51<00:58,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10423/10702 [35:51<00:57,  4.84it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10425/10702 [35:51<00:57,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10427/10702 [35:51<00:56,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10429/10702 [35:51<00:56,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10431/10702 [35:51<00:55,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  97% 10433/10702 [35:52<00:55,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10435/10702 [35:52<00:55,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10437/10702 [35:52<00:54,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10439/10702 [35:52<00:54,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10441/10702 [35:52<00:53,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10443/10702 [35:52<00:53,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10445/10702 [35:52<00:52,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10447/10702 [35:52<00:52,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10449/10702 [35:53<00:52,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10451/10702 [35:53<00:51,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10453/10702 [35:53<00:51,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10455/10702 [35:53<00:50,  4.85it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10457/10702 [35:53<00:50,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10459/10702 [35:53<00:50,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10461/10702 [35:53<00:49,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10463/10702 [35:54<00:49,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10465/10702 [35:54<00:48,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10467/10702 [35:54<00:48,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10469/10702 [35:54<00:47,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10471/10702 [35:54<00:47,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10473/10702 [35:54<00:47,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10475/10702 [35:54<00:46,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10477/10702 [35:55<00:46,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10479/10702 [35:55<00:45,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10481/10702 [35:55<00:45,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10483/10702 [35:55<00:45,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10485/10702 [35:55<00:44,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10487/10702 [35:55<00:44,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10489/10702 [35:55<00:43,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10491/10702 [35:55<00:43,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10493/10702 [35:56<00:42,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10495/10702 [35:56<00:42,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10497/10702 [35:56<00:42,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10499/10702 [35:56<00:41,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10501/10702 [35:56<00:41,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10503/10702 [35:56<00:40,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10505/10702 [35:56<00:40,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10507/10702 [35:57<00:40,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10509/10702 [35:57<00:39,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10511/10702 [35:57<00:39,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10513/10702 [35:57<00:38,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10515/10702 [35:57<00:38,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10517/10702 [35:57<00:37,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10519/10702 [35:57<00:37,  4.87it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10521/10702 [35:58<00:37,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10523/10702 [35:58<00:36,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10525/10702 [35:58<00:36,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10527/10702 [35:58<00:35,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10529/10702 [35:58<00:35,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10531/10702 [35:58<00:35,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10533/10702 [35:58<00:34,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10535/10702 [35:59<00:34,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10537/10702 [35:59<00:33,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10539/10702 [35:59<00:33,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  98% 10541/10702 [35:59<00:32,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10543/10702 [35:59<00:32,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10545/10702 [35:59<00:32,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10547/10702 [35:59<00:31,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10549/10702 [35:59<00:31,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10551/10702 [36:00<00:30,  4.88it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10553/10702 [36:00<00:30,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10555/10702 [36:00<00:30,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10557/10702 [36:00<00:29,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10559/10702 [36:00<00:29,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10561/10702 [36:00<00:28,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10563/10702 [36:00<00:28,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10565/10702 [36:01<00:28,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10567/10702 [36:01<00:27,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10569/10702 [36:01<00:27,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10571/10702 [36:01<00:26,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10573/10702 [36:01<00:26,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10575/10702 [36:01<00:25,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10577/10702 [36:01<00:25,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10579/10702 [36:02<00:25,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10581/10702 [36:02<00:24,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10583/10702 [36:02<00:24,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10585/10702 [36:02<00:23,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10587/10702 [36:02<00:23,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10589/10702 [36:02<00:23,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10591/10702 [36:02<00:22,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10593/10702 [36:02<00:22,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10595/10702 [36:03<00:21,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10597/10702 [36:03<00:21,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10599/10702 [36:03<00:21,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10601/10702 [36:03<00:20,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10603/10702 [36:03<00:20,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10605/10702 [36:03<00:19,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10607/10702 [36:03<00:19,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10609/10702 [36:04<00:18,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10611/10702 [36:04<00:18,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10613/10702 [36:04<00:18,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10615/10702 [36:04<00:17,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10617/10702 [36:04<00:17,  4.90it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10619/10702 [36:04<00:16,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10621/10702 [36:04<00:16,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10623/10702 [36:05<00:16,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10625/10702 [36:05<00:15,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10627/10702 [36:05<00:15,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10629/10702 [36:05<00:14,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10631/10702 [36:05<00:14,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10633/10702 [36:05<00:14,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10635/10702 [36:05<00:13,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10637/10702 [36:05<00:13,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10639/10702 [36:06<00:12,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10641/10702 [36:06<00:12,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10643/10702 [36:06<00:12,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10645/10702 [36:06<00:11,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37:  99% 10647/10702 [36:06<00:11,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10649/10702 [36:06<00:10,  4.91it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10651/10702 [36:06<00:10,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10653/10702 [36:07<00:09,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10655/10702 [36:07<00:09,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10657/10702 [36:07<00:09,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10659/10702 [36:07<00:08,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10661/10702 [36:07<00:08,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10663/10702 [36:07<00:07,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10665/10702 [36:07<00:07,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10667/10702 [36:08<00:07,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10669/10702 [36:08<00:06,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10671/10702 [36:08<00:06,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10673/10702 [36:08<00:05,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10675/10702 [36:08<00:05,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10677/10702 [36:08<00:05,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10679/10702 [36:08<00:04,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10681/10702 [36:08<00:04,  4.92it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10683/10702 [36:09<00:03,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10685/10702 [36:09<00:03,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10687/10702 [36:09<00:03,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10689/10702 [36:09<00:02,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10691/10702 [36:09<00:02,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10693/10702 [36:09<00:01,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10695/10702 [36:09<00:01,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10697/10702 [36:10<00:01,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10699/10702 [36:10<00:00,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10701/10702 [36:10<00:00,  4.93it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Validating: 100% 2141/2141 [02:25<00:00, 15.20it/s]\u001b[AEpoch 37, global step 325317: val_loss reached 2.49694 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=37-val_loss=2.497.ckpt\" as top 1\n",
            "INFO:lightning:Epoch 37, global step 325317: val_loss reached 2.49694 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=37-val_loss=2.497.ckpt\" as top 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1320878080 bytes == 0x55e9d99a8000 @  0x7f036201b615 0x55e896508cdc 0x55e8965e852a 0x55e89650f76c 0x7f03370a1a94 0x7f03370a3864 0x7f0337073590 0x7f03279b9465 0x7f03279b59ca 0x7f03279ba609 0x7f0337076f2b 0x7f0336cfc200 0x55e89650c8a8 0x55e89657ffd5 0x55e89657a7ad 0x55e89650d3ea 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657b60e 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b60e 0x55e89650d30a\n",
            "tcmalloc: large alloc 1651097600 bytes == 0x55ea28558000 @  0x7f036201b615 0x55e896508cdc 0x55e8965e852a 0x55e89650f76c 0x7f03370a1a94 0x7f03370a3864 0x7f0337073590 0x7f03279b9465 0x7f03279b59ca 0x7f03279ba609 0x7f0337076f2b 0x7f0336cfc200 0x55e89650c8a8 0x55e89657ffd5 0x55e89657a7ad 0x55e89650d3ea 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657b60e 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b60e 0x55e89650d30a\n",
            "tcmalloc: large alloc 1651097600 bytes == 0x55ea28558000 @  0x7f036201b615 0x55e896508cdc 0x55e8965e852a 0x55e89650f76c 0x7f03370a1a94 0x7f03370a3864 0x7f0337073590 0x7f03279b9465 0x7f03279b59ca 0x7f03279ba609 0x7f0337076f2b 0x7f0336cfc200 0x55e89650c8a8 0x55e89657ffd5 0x55e89657a7ad 0x55e89650d3ea 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657b60e 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b60e 0x55e89650d30a\n",
            "Epoch 37: 100% 10702/10702 [36:30<00:00,  4.89it/s, loss=0.0844, v_num=9, val_loss=2.57, train_loss=0.0876]\n",
            "Epoch 37: 100% 10702/10702 [36:40<00:00,  4.86it/s, loss=0.0844, v_num=9, val_loss=2.5, train_loss=0.103]  \n",
            "Epoch 38:  80% 8561/10702 [34:05<08:31,  4.18it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 38:  80% 8563/10702 [34:06<08:31,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8565/10702 [34:06<08:30,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8567/10702 [34:06<08:29,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8569/10702 [34:06<08:29,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8571/10702 [34:06<08:28,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8573/10702 [34:06<08:28,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8575/10702 [34:06<08:27,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8577/10702 [34:07<08:27,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8579/10702 [34:07<08:26,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8581/10702 [34:07<08:26,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8583/10702 [34:07<08:25,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8585/10702 [34:07<08:24,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8587/10702 [34:07<08:24,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8589/10702 [34:07<08:23,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8591/10702 [34:07<08:23,  4.19it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8593/10702 [34:08<08:22,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8595/10702 [34:08<08:22,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8597/10702 [34:08<08:21,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8599/10702 [34:08<08:20,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8601/10702 [34:08<08:20,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8603/10702 [34:08<08:19,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8605/10702 [34:08<08:19,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8607/10702 [34:09<08:18,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8609/10702 [34:09<08:18,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8611/10702 [34:09<08:17,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8613/10702 [34:09<08:17,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  80% 8615/10702 [34:09<08:16,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8617/10702 [34:09<08:15,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8619/10702 [34:09<08:15,  4.20it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8621/10702 [34:10<08:14,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8623/10702 [34:10<08:14,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8625/10702 [34:10<08:13,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8627/10702 [34:10<08:13,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8629/10702 [34:10<08:12,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8631/10702 [34:10<08:12,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8633/10702 [34:10<08:11,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8635/10702 [34:10<08:10,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8637/10702 [34:11<08:10,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8639/10702 [34:11<08:09,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8641/10702 [34:11<08:09,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8643/10702 [34:11<08:08,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8645/10702 [34:11<08:08,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8647/10702 [34:11<08:07,  4.21it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8649/10702 [34:11<08:07,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8651/10702 [34:12<08:06,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8653/10702 [34:12<08:05,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8655/10702 [34:12<08:05,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8657/10702 [34:12<08:04,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8659/10702 [34:12<08:04,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8661/10702 [34:12<08:03,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8663/10702 [34:12<08:03,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8665/10702 [34:13<08:02,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8667/10702 [34:13<08:02,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8669/10702 [34:13<08:01,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8671/10702 [34:13<08:00,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8673/10702 [34:13<08:00,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8675/10702 [34:13<07:59,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8677/10702 [34:13<07:59,  4.22it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8679/10702 [34:13<07:58,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8681/10702 [34:14<07:58,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8683/10702 [34:14<07:57,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8685/10702 [34:14<07:57,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8687/10702 [34:14<07:56,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8689/10702 [34:14<07:56,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8691/10702 [34:14<07:55,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8693/10702 [34:14<07:54,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8695/10702 [34:15<07:54,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8697/10702 [34:15<07:53,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8699/10702 [34:15<07:53,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8701/10702 [34:15<07:52,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8703/10702 [34:15<07:52,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8705/10702 [34:15<07:51,  4.23it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8707/10702 [34:15<07:51,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8709/10702 [34:16<07:50,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8711/10702 [34:16<07:49,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8713/10702 [34:16<07:49,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8715/10702 [34:16<07:48,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8717/10702 [34:16<07:48,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8719/10702 [34:16<07:47,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  81% 8721/10702 [34:16<07:47,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8723/10702 [34:16<07:46,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8725/10702 [34:17<07:46,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8727/10702 [34:17<07:45,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8729/10702 [34:17<07:45,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8731/10702 [34:17<07:44,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8733/10702 [34:17<07:43,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8735/10702 [34:17<07:43,  4.24it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8737/10702 [34:17<07:42,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8739/10702 [34:18<07:42,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8741/10702 [34:18<07:41,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8743/10702 [34:18<07:41,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8745/10702 [34:18<07:40,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8747/10702 [34:18<07:40,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8749/10702 [34:18<07:39,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8751/10702 [34:18<07:39,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8753/10702 [34:19<07:38,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8755/10702 [34:19<07:37,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8757/10702 [34:19<07:37,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8759/10702 [34:19<07:36,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8761/10702 [34:19<07:36,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8763/10702 [34:19<07:35,  4.25it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8765/10702 [34:19<07:35,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8767/10702 [34:19<07:34,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8769/10702 [34:20<07:34,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8771/10702 [34:20<07:33,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8773/10702 [34:20<07:33,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8775/10702 [34:20<07:32,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8777/10702 [34:20<07:31,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8779/10702 [34:20<07:31,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8781/10702 [34:20<07:30,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8783/10702 [34:21<07:30,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8785/10702 [34:21<07:29,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8787/10702 [34:21<07:29,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8789/10702 [34:21<07:28,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8791/10702 [34:21<07:28,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8793/10702 [34:21<07:27,  4.26it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8795/10702 [34:21<07:27,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8797/10702 [34:22<07:26,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8799/10702 [34:22<07:25,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8801/10702 [34:22<07:25,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8803/10702 [34:22<07:24,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8805/10702 [34:22<07:24,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8807/10702 [34:22<07:23,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8809/10702 [34:22<07:23,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8811/10702 [34:22<07:22,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8813/10702 [34:23<07:22,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8815/10702 [34:23<07:21,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8817/10702 [34:23<07:21,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8819/10702 [34:23<07:20,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8821/10702 [34:23<07:20,  4.27it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8823/10702 [34:23<07:19,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8825/10702 [34:23<07:18,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8827/10702 [34:24<07:18,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  82% 8829/10702 [34:24<07:17,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8831/10702 [34:24<07:17,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8833/10702 [34:24<07:16,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8835/10702 [34:24<07:16,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8837/10702 [34:24<07:15,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8839/10702 [34:24<07:15,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8841/10702 [34:25<07:14,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8843/10702 [34:25<07:14,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8845/10702 [34:25<07:13,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8847/10702 [34:25<07:13,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8849/10702 [34:25<07:12,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8851/10702 [34:25<07:12,  4.28it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8853/10702 [34:25<07:11,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8855/10702 [34:25<07:10,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8857/10702 [34:26<07:10,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8859/10702 [34:26<07:09,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8861/10702 [34:26<07:09,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8863/10702 [34:26<07:08,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8865/10702 [34:26<07:08,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8867/10702 [34:26<07:07,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8869/10702 [34:26<07:07,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8871/10702 [34:27<07:06,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8873/10702 [34:27<07:06,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8875/10702 [34:27<07:05,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8877/10702 [34:27<07:05,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8879/10702 [34:27<07:04,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8881/10702 [34:27<07:03,  4.29it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8883/10702 [34:27<07:03,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8885/10702 [34:28<07:02,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8887/10702 [34:28<07:02,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8889/10702 [34:28<07:01,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8891/10702 [34:28<07:01,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8893/10702 [34:28<07:00,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8895/10702 [34:28<07:00,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8897/10702 [34:28<06:59,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8899/10702 [34:28<06:59,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8901/10702 [34:29<06:58,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8903/10702 [34:29<06:58,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8905/10702 [34:29<06:57,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8907/10702 [34:29<06:57,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8909/10702 [34:29<06:56,  4.30it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8911/10702 [34:29<06:56,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8913/10702 [34:29<06:55,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8915/10702 [34:30<06:54,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8917/10702 [34:30<06:54,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8919/10702 [34:30<06:53,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8921/10702 [34:30<06:53,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8923/10702 [34:30<06:52,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8925/10702 [34:30<06:52,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8927/10702 [34:30<06:51,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8929/10702 [34:31<06:51,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8931/10702 [34:31<06:50,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8933/10702 [34:31<06:50,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  83% 8935/10702 [34:31<06:49,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8937/10702 [34:31<06:49,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8939/10702 [34:31<06:48,  4.31it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8941/10702 [34:31<06:48,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8943/10702 [34:31<06:47,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8945/10702 [34:32<06:47,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8947/10702 [34:32<06:46,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8949/10702 [34:32<06:45,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8951/10702 [34:32<06:45,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8953/10702 [34:32<06:44,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8955/10702 [34:32<06:44,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8957/10702 [34:32<06:43,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8959/10702 [34:33<06:43,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8961/10702 [34:33<06:42,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8963/10702 [34:33<06:42,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8965/10702 [34:33<06:41,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8967/10702 [34:33<06:41,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8969/10702 [34:33<06:40,  4.32it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8971/10702 [34:33<06:40,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8973/10702 [34:34<06:39,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8975/10702 [34:34<06:39,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8977/10702 [34:34<06:38,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8979/10702 [34:34<06:38,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8981/10702 [34:34<06:37,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8983/10702 [34:34<06:37,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8985/10702 [34:34<06:36,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8987/10702 [34:34<06:35,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8989/10702 [34:35<06:35,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8991/10702 [34:35<06:34,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8993/10702 [34:35<06:34,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8995/10702 [34:35<06:33,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8997/10702 [34:35<06:33,  4.33it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 8999/10702 [34:35<06:32,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9001/10702 [34:35<06:32,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9003/10702 [34:36<06:31,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9005/10702 [34:36<06:31,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9007/10702 [34:36<06:30,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9009/10702 [34:36<06:30,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9011/10702 [34:36<06:29,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9013/10702 [34:36<06:29,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9015/10702 [34:36<06:28,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9017/10702 [34:37<06:28,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9019/10702 [34:37<06:27,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9021/10702 [34:37<06:27,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9023/10702 [34:37<06:26,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9025/10702 [34:37<06:26,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9027/10702 [34:37<06:25,  4.34it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9029/10702 [34:37<06:25,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9031/10702 [34:37<06:24,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9033/10702 [34:38<06:23,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9035/10702 [34:38<06:23,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9037/10702 [34:38<06:22,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9039/10702 [34:38<06:22,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9041/10702 [34:38<06:21,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  84% 9043/10702 [34:38<06:21,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9045/10702 [34:38<06:20,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9047/10702 [34:39<06:20,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9049/10702 [34:39<06:19,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9051/10702 [34:39<06:19,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9053/10702 [34:39<06:18,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9055/10702 [34:39<06:18,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9057/10702 [34:39<06:17,  4.35it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9059/10702 [34:39<06:17,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9061/10702 [34:40<06:16,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9063/10702 [34:40<06:16,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9065/10702 [34:40<06:15,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9067/10702 [34:40<06:15,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9069/10702 [34:40<06:14,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9071/10702 [34:40<06:14,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9073/10702 [34:40<06:13,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9075/10702 [34:40<06:13,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9077/10702 [34:41<06:12,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9079/10702 [34:41<06:12,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9081/10702 [34:41<06:11,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9083/10702 [34:41<06:11,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9085/10702 [34:41<06:10,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9087/10702 [34:41<06:09,  4.36it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9089/10702 [34:41<06:09,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9091/10702 [34:42<06:08,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9093/10702 [34:42<06:08,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9095/10702 [34:42<06:07,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9097/10702 [34:42<06:07,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9099/10702 [34:42<06:06,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9101/10702 [34:42<06:06,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9103/10702 [34:42<06:05,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9105/10702 [34:43<06:05,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9107/10702 [34:43<06:04,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9109/10702 [34:43<06:04,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9111/10702 [34:43<06:03,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9113/10702 [34:43<06:03,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9115/10702 [34:43<06:02,  4.37it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9117/10702 [34:43<06:02,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9119/10702 [34:44<06:01,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9121/10702 [34:44<06:01,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9123/10702 [34:44<06:00,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9125/10702 [34:44<06:00,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9127/10702 [34:44<05:59,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9129/10702 [34:44<05:59,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9131/10702 [34:44<05:58,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9133/10702 [34:44<05:58,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9135/10702 [34:45<05:57,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9137/10702 [34:45<05:57,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9139/10702 [34:45<05:56,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9141/10702 [34:45<05:56,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9143/10702 [34:45<05:55,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9145/10702 [34:45<05:55,  4.38it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9147/10702 [34:45<05:54,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  85% 9149/10702 [34:46<05:54,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9151/10702 [34:46<05:53,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9153/10702 [34:46<05:53,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9155/10702 [34:46<05:52,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9157/10702 [34:46<05:52,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9159/10702 [34:46<05:51,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9161/10702 [34:46<05:51,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9163/10702 [34:47<05:50,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9165/10702 [34:47<05:50,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9167/10702 [34:47<05:49,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9169/10702 [34:47<05:49,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9171/10702 [34:47<05:48,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9173/10702 [34:47<05:47,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9175/10702 [34:47<05:47,  4.39it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9177/10702 [34:47<05:46,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9179/10702 [34:48<05:46,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9181/10702 [34:48<05:45,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9183/10702 [34:48<05:45,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9185/10702 [34:48<05:44,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9187/10702 [34:48<05:44,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9189/10702 [34:48<05:43,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9191/10702 [34:48<05:43,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9193/10702 [34:49<05:42,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9195/10702 [34:49<05:42,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9197/10702 [34:49<05:41,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9199/10702 [34:49<05:41,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9201/10702 [34:49<05:40,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9203/10702 [34:49<05:40,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9205/10702 [34:49<05:39,  4.40it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9207/10702 [34:50<05:39,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9209/10702 [34:50<05:38,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9211/10702 [34:50<05:38,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9213/10702 [34:50<05:37,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9215/10702 [34:50<05:37,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9217/10702 [34:50<05:36,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9219/10702 [34:50<05:36,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9221/10702 [34:50<05:35,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9223/10702 [34:51<05:35,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9225/10702 [34:51<05:34,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9227/10702 [34:51<05:34,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9229/10702 [34:51<05:33,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9231/10702 [34:51<05:33,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9233/10702 [34:51<05:32,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9235/10702 [34:51<05:32,  4.41it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9237/10702 [34:52<05:31,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9239/10702 [34:52<05:31,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9241/10702 [34:52<05:30,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9243/10702 [34:52<05:30,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9245/10702 [34:52<05:29,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9247/10702 [34:52<05:29,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9249/10702 [34:52<05:28,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9251/10702 [34:53<05:28,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9253/10702 [34:53<05:27,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9255/10702 [34:53<05:27,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  86% 9257/10702 [34:53<05:26,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9259/10702 [34:53<05:26,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9261/10702 [34:53<05:25,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9263/10702 [34:53<05:25,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9265/10702 [34:53<05:24,  4.42it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9267/10702 [34:54<05:24,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9269/10702 [34:54<05:23,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9271/10702 [34:54<05:23,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9273/10702 [34:54<05:22,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9275/10702 [34:54<05:22,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9277/10702 [34:54<05:21,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9279/10702 [34:54<05:21,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9281/10702 [34:55<05:20,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9283/10702 [34:55<05:20,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9285/10702 [34:55<05:19,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9287/10702 [34:55<05:19,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9289/10702 [34:55<05:18,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9291/10702 [34:55<05:18,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9293/10702 [34:55<05:17,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9295/10702 [34:56<05:17,  4.43it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9297/10702 [34:56<05:16,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9299/10702 [34:56<05:16,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9301/10702 [34:56<05:15,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9303/10702 [34:56<05:15,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9305/10702 [34:56<05:14,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9307/10702 [34:56<05:14,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9309/10702 [34:56<05:13,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9311/10702 [34:57<05:13,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9313/10702 [34:57<05:12,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9315/10702 [34:57<05:12,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9317/10702 [34:57<05:11,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9319/10702 [34:57<05:11,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9321/10702 [34:57<05:10,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9323/10702 [34:57<05:10,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9325/10702 [34:58<05:09,  4.44it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9327/10702 [34:58<05:09,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9329/10702 [34:58<05:08,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9331/10702 [34:58<05:08,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9333/10702 [34:58<05:07,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9335/10702 [34:58<05:07,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9337/10702 [34:58<05:06,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9339/10702 [34:59<05:06,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9341/10702 [34:59<05:05,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9343/10702 [34:59<05:05,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9345/10702 [34:59<05:04,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9347/10702 [34:59<05:04,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9349/10702 [34:59<05:03,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9351/10702 [34:59<05:03,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9353/10702 [34:59<05:02,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9355/10702 [35:00<05:02,  4.45it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9357/10702 [35:00<05:01,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9359/10702 [35:00<05:01,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9361/10702 [35:00<05:00,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  87% 9363/10702 [35:00<05:00,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9365/10702 [35:00<04:59,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9367/10702 [35:00<04:59,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9369/10702 [35:01<04:58,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9371/10702 [35:01<04:58,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9373/10702 [35:01<04:57,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9375/10702 [35:01<04:57,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9377/10702 [35:01<04:56,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9379/10702 [35:01<04:56,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9381/10702 [35:01<04:55,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9383/10702 [35:02<04:55,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9385/10702 [35:02<04:55,  4.46it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9387/10702 [35:02<04:54,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9389/10702 [35:02<04:54,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9391/10702 [35:02<04:53,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9393/10702 [35:02<04:53,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9395/10702 [35:02<04:52,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9397/10702 [35:03<04:52,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9399/10702 [35:03<04:51,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9401/10702 [35:03<04:51,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9403/10702 [35:03<04:50,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9405/10702 [35:03<04:50,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9407/10702 [35:03<04:49,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9409/10702 [35:03<04:49,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9411/10702 [35:03<04:48,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9413/10702 [35:04<04:48,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9415/10702 [35:04<04:47,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9417/10702 [35:04<04:47,  4.47it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9419/10702 [35:04<04:46,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9421/10702 [35:04<04:46,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9423/10702 [35:04<04:45,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9425/10702 [35:04<04:45,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9427/10702 [35:05<04:44,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9429/10702 [35:05<04:44,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9431/10702 [35:05<04:43,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9433/10702 [35:05<04:43,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9435/10702 [35:05<04:42,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9437/10702 [35:05<04:42,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9439/10702 [35:05<04:41,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9441/10702 [35:06<04:41,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9443/10702 [35:06<04:40,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9445/10702 [35:06<04:40,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9447/10702 [35:06<04:39,  4.48it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9449/10702 [35:06<04:39,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9451/10702 [35:06<04:38,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9453/10702 [35:06<04:38,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9455/10702 [35:06<04:37,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9457/10702 [35:07<04:37,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9459/10702 [35:07<04:36,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9461/10702 [35:07<04:36,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9463/10702 [35:07<04:35,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9465/10702 [35:07<04:35,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9467/10702 [35:07<04:34,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9469/10702 [35:07<04:34,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  88% 9471/10702 [35:08<04:33,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9473/10702 [35:08<04:33,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9475/10702 [35:08<04:33,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9477/10702 [35:08<04:32,  4.49it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9479/10702 [35:08<04:32,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9481/10702 [35:08<04:31,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9483/10702 [35:08<04:31,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9485/10702 [35:09<04:30,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9487/10702 [35:09<04:30,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9489/10702 [35:09<04:29,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9491/10702 [35:09<04:29,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9493/10702 [35:09<04:28,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9495/10702 [35:09<04:28,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9497/10702 [35:09<04:27,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9499/10702 [35:09<04:27,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9501/10702 [35:10<04:26,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9503/10702 [35:10<04:26,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9505/10702 [35:10<04:25,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9507/10702 [35:10<04:25,  4.50it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9509/10702 [35:10<04:24,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9511/10702 [35:10<04:24,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9513/10702 [35:10<04:23,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9515/10702 [35:11<04:23,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9517/10702 [35:11<04:22,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9519/10702 [35:11<04:22,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9521/10702 [35:11<04:21,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9523/10702 [35:11<04:21,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9525/10702 [35:11<04:20,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9527/10702 [35:11<04:20,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9529/10702 [35:12<04:19,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9531/10702 [35:12<04:19,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9533/10702 [35:12<04:19,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9535/10702 [35:12<04:18,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9537/10702 [35:12<04:18,  4.51it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9539/10702 [35:12<04:17,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9541/10702 [35:12<04:17,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9543/10702 [35:12<04:16,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9545/10702 [35:13<04:16,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9547/10702 [35:13<04:15,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9549/10702 [35:13<04:15,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9551/10702 [35:13<04:14,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9553/10702 [35:13<04:14,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9555/10702 [35:13<04:13,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9557/10702 [35:13<04:13,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9559/10702 [35:14<04:12,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9561/10702 [35:14<04:12,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9563/10702 [35:14<04:11,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9565/10702 [35:14<04:11,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9567/10702 [35:14<04:10,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9569/10702 [35:14<04:10,  4.52it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9571/10702 [35:14<04:09,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9573/10702 [35:15<04:09,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9575/10702 [35:15<04:08,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  89% 9577/10702 [35:15<04:08,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9579/10702 [35:15<04:08,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9581/10702 [35:15<04:07,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9583/10702 [35:15<04:07,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9585/10702 [35:15<04:06,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9587/10702 [35:15<04:06,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9589/10702 [35:16<04:05,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9591/10702 [35:16<04:05,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9593/10702 [35:16<04:04,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9595/10702 [35:16<04:04,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9597/10702 [35:16<04:03,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9599/10702 [35:16<04:03,  4.53it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9601/10702 [35:16<04:02,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9603/10702 [35:17<04:02,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9605/10702 [35:17<04:01,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9607/10702 [35:17<04:01,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9609/10702 [35:17<04:00,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9611/10702 [35:17<04:00,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9613/10702 [35:17<03:59,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9615/10702 [35:17<03:59,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9617/10702 [35:18<03:58,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9619/10702 [35:18<03:58,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9621/10702 [35:18<03:58,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9623/10702 [35:18<03:57,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9625/10702 [35:18<03:57,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9627/10702 [35:18<03:56,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9629/10702 [35:18<03:56,  4.54it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9631/10702 [35:18<03:55,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9633/10702 [35:19<03:55,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9635/10702 [35:19<03:54,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9637/10702 [35:19<03:54,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9639/10702 [35:19<03:53,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9641/10702 [35:19<03:53,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9643/10702 [35:19<03:52,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9645/10702 [35:19<03:52,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9647/10702 [35:20<03:51,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9649/10702 [35:20<03:51,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9651/10702 [35:20<03:50,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9653/10702 [35:20<03:50,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9655/10702 [35:20<03:49,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9657/10702 [35:20<03:49,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9659/10702 [35:20<03:49,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9661/10702 [35:21<03:48,  4.55it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9663/10702 [35:21<03:48,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9665/10702 [35:21<03:47,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9667/10702 [35:21<03:47,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9669/10702 [35:21<03:46,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9671/10702 [35:21<03:46,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9673/10702 [35:21<03:45,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9675/10702 [35:22<03:45,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9677/10702 [35:22<03:44,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9679/10702 [35:22<03:44,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9681/10702 [35:22<03:43,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9683/10702 [35:22<03:43,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  90% 9685/10702 [35:22<03:42,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9687/10702 [35:22<03:42,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9689/10702 [35:22<03:41,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9691/10702 [35:23<03:41,  4.56it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9693/10702 [35:23<03:41,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9695/10702 [35:23<03:40,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9697/10702 [35:23<03:40,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9699/10702 [35:23<03:39,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9701/10702 [35:23<03:39,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9703/10702 [35:23<03:38,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9705/10702 [35:24<03:38,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9707/10702 [35:24<03:37,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9709/10702 [35:24<03:37,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9711/10702 [35:24<03:36,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9713/10702 [35:24<03:36,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9715/10702 [35:24<03:35,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9717/10702 [35:24<03:35,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9719/10702 [35:25<03:34,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9721/10702 [35:25<03:34,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9723/10702 [35:25<03:33,  4.57it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9725/10702 [35:25<03:33,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9727/10702 [35:25<03:33,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9729/10702 [35:25<03:32,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9731/10702 [35:25<03:32,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9733/10702 [35:25<03:31,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9735/10702 [35:26<03:31,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9737/10702 [35:26<03:30,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9739/10702 [35:26<03:30,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9741/10702 [35:26<03:29,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9743/10702 [35:26<03:29,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9745/10702 [35:26<03:28,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9747/10702 [35:26<03:28,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9749/10702 [35:27<03:27,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9751/10702 [35:27<03:27,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9753/10702 [35:27<03:26,  4.58it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9755/10702 [35:27<03:26,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9757/10702 [35:27<03:26,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9759/10702 [35:27<03:25,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9761/10702 [35:27<03:25,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9763/10702 [35:28<03:24,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9765/10702 [35:28<03:24,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9767/10702 [35:28<03:23,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9769/10702 [35:28<03:23,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9771/10702 [35:28<03:22,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9773/10702 [35:28<03:22,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9775/10702 [35:28<03:21,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9777/10702 [35:28<03:21,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9779/10702 [35:29<03:20,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9781/10702 [35:29<03:20,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9783/10702 [35:29<03:20,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9785/10702 [35:29<03:19,  4.59it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9787/10702 [35:29<03:19,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9789/10702 [35:29<03:18,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  91% 9791/10702 [35:29<03:18,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9793/10702 [35:30<03:17,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9795/10702 [35:30<03:17,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9797/10702 [35:30<03:16,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9799/10702 [35:30<03:16,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9801/10702 [35:30<03:15,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9803/10702 [35:30<03:15,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9805/10702 [35:30<03:14,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9807/10702 [35:31<03:14,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9809/10702 [35:31<03:14,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9811/10702 [35:31<03:13,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9813/10702 [35:31<03:13,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9815/10702 [35:31<03:12,  4.60it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9817/10702 [35:31<03:12,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9819/10702 [35:31<03:11,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9821/10702 [35:31<03:11,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9823/10702 [35:32<03:10,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9825/10702 [35:32<03:10,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9827/10702 [35:32<03:09,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9829/10702 [35:32<03:09,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9831/10702 [35:32<03:08,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9833/10702 [35:32<03:08,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9835/10702 [35:32<03:08,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9837/10702 [35:33<03:07,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9839/10702 [35:33<03:07,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9841/10702 [35:33<03:06,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9843/10702 [35:33<03:06,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9845/10702 [35:33<03:05,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9847/10702 [35:33<03:05,  4.61it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9849/10702 [35:33<03:04,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9851/10702 [35:34<03:04,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9853/10702 [35:34<03:03,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9855/10702 [35:34<03:03,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9857/10702 [35:34<03:02,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9859/10702 [35:34<03:02,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9861/10702 [35:34<03:02,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9863/10702 [35:34<03:01,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9865/10702 [35:34<03:01,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9867/10702 [35:35<03:00,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9869/10702 [35:35<03:00,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9871/10702 [35:35<02:59,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9873/10702 [35:35<02:59,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9875/10702 [35:35<02:58,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9877/10702 [35:35<02:58,  4.62it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9879/10702 [35:35<02:57,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9881/10702 [35:36<02:57,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9883/10702 [35:36<02:57,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9885/10702 [35:36<02:56,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9887/10702 [35:36<02:56,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9889/10702 [35:36<02:55,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9891/10702 [35:36<02:55,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9893/10702 [35:36<02:54,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9895/10702 [35:37<02:54,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9897/10702 [35:37<02:53,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  92% 9899/10702 [35:37<02:53,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9901/10702 [35:37<02:52,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9903/10702 [35:37<02:52,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9905/10702 [35:37<02:52,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9907/10702 [35:37<02:51,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9909/10702 [35:38<02:51,  4.63it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9911/10702 [35:38<02:50,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9913/10702 [35:38<02:50,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9915/10702 [35:38<02:49,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9917/10702 [35:38<02:49,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9919/10702 [35:38<02:48,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9921/10702 [35:38<02:48,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9923/10702 [35:38<02:47,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9925/10702 [35:39<02:47,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9927/10702 [35:39<02:47,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9929/10702 [35:39<02:46,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9931/10702 [35:39<02:46,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9933/10702 [35:39<02:45,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9935/10702 [35:39<02:45,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9937/10702 [35:39<02:44,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9939/10702 [35:40<02:44,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9941/10702 [35:40<02:43,  4.64it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9943/10702 [35:40<02:43,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9945/10702 [35:40<02:42,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9947/10702 [35:40<02:42,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9949/10702 [35:40<02:42,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9951/10702 [35:40<02:41,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9953/10702 [35:41<02:41,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9955/10702 [35:41<02:40,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9957/10702 [35:41<02:40,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9959/10702 [35:41<02:39,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9961/10702 [35:41<02:39,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9963/10702 [35:41<02:38,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9965/10702 [35:41<02:38,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9967/10702 [35:41<02:37,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9969/10702 [35:42<02:37,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9971/10702 [35:42<02:37,  4.65it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9973/10702 [35:42<02:36,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9975/10702 [35:42<02:36,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9977/10702 [35:42<02:35,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9979/10702 [35:42<02:35,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9981/10702 [35:42<02:34,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9983/10702 [35:43<02:34,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9985/10702 [35:43<02:33,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9987/10702 [35:43<02:33,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9989/10702 [35:43<02:32,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9991/10702 [35:43<02:32,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9993/10702 [35:43<02:32,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9995/10702 [35:43<02:31,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9997/10702 [35:44<02:31,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 9999/10702 [35:44<02:30,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 10001/10702 [35:44<02:30,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 10003/10702 [35:44<02:29,  4.66it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  93% 10005/10702 [35:44<02:29,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10007/10702 [35:44<02:28,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10009/10702 [35:44<02:28,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10011/10702 [35:44<02:28,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10013/10702 [35:45<02:27,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10015/10702 [35:45<02:27,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10017/10702 [35:45<02:26,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10019/10702 [35:45<02:26,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10021/10702 [35:45<02:25,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10023/10702 [35:45<02:25,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10025/10702 [35:45<02:24,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10027/10702 [35:46<02:24,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10029/10702 [35:46<02:24,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10031/10702 [35:46<02:23,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10033/10702 [35:46<02:23,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10035/10702 [35:46<02:22,  4.67it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10037/10702 [35:46<02:22,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10039/10702 [35:46<02:21,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10041/10702 [35:47<02:21,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10043/10702 [35:47<02:20,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10045/10702 [35:47<02:20,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10047/10702 [35:47<02:19,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10049/10702 [35:47<02:19,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10051/10702 [35:47<02:19,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10053/10702 [35:47<02:18,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10055/10702 [35:47<02:18,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10057/10702 [35:48<02:17,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10059/10702 [35:48<02:17,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10061/10702 [35:48<02:16,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10063/10702 [35:48<02:16,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10065/10702 [35:48<02:15,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10067/10702 [35:48<02:15,  4.68it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10069/10702 [35:48<02:15,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10071/10702 [35:49<02:14,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10073/10702 [35:49<02:14,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10075/10702 [35:49<02:13,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10077/10702 [35:49<02:13,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10079/10702 [35:49<02:12,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10081/10702 [35:49<02:12,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10083/10702 [35:49<02:11,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10085/10702 [35:50<02:11,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10087/10702 [35:50<02:11,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10089/10702 [35:50<02:10,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10091/10702 [35:50<02:10,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10093/10702 [35:50<02:09,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10095/10702 [35:50<02:09,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10097/10702 [35:50<02:08,  4.69it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10099/10702 [35:50<02:08,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10101/10702 [35:51<02:07,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10103/10702 [35:51<02:07,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10105/10702 [35:51<02:07,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10107/10702 [35:51<02:06,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10109/10702 [35:51<02:06,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10111/10702 [35:51<02:05,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  94% 10113/10702 [35:51<02:05,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10115/10702 [35:52<02:04,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10117/10702 [35:52<02:04,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10119/10702 [35:52<02:04,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10121/10702 [35:52<02:03,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10123/10702 [35:52<02:03,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10125/10702 [35:52<02:02,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10127/10702 [35:52<02:02,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10129/10702 [35:53<02:01,  4.70it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10131/10702 [35:53<02:01,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10133/10702 [35:53<02:00,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10135/10702 [35:53<02:00,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10137/10702 [35:53<02:00,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10139/10702 [35:53<01:59,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10141/10702 [35:53<01:59,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10143/10702 [35:54<01:58,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10145/10702 [35:54<01:58,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10147/10702 [35:54<01:57,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10149/10702 [35:54<01:57,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10151/10702 [35:54<01:56,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10153/10702 [35:54<01:56,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10155/10702 [35:54<01:56,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10157/10702 [35:54<01:55,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10159/10702 [35:55<01:55,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10161/10702 [35:55<01:54,  4.71it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10163/10702 [35:55<01:54,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10165/10702 [35:55<01:53,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10167/10702 [35:55<01:53,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10169/10702 [35:55<01:52,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10171/10702 [35:55<01:52,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10173/10702 [35:56<01:52,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10175/10702 [35:56<01:51,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10177/10702 [35:56<01:51,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10179/10702 [35:56<01:50,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10181/10702 [35:56<01:50,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10183/10702 [35:56<01:49,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10185/10702 [35:56<01:49,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10187/10702 [35:57<01:49,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10189/10702 [35:57<01:48,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10191/10702 [35:57<01:48,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10193/10702 [35:57<01:47,  4.72it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10195/10702 [35:57<01:47,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10197/10702 [35:57<01:46,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10199/10702 [35:57<01:46,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10201/10702 [35:57<01:45,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10203/10702 [35:58<01:45,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10205/10702 [35:58<01:45,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10207/10702 [35:58<01:44,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10209/10702 [35:58<01:44,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10211/10702 [35:58<01:43,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10213/10702 [35:58<01:43,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10215/10702 [35:58<01:42,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10217/10702 [35:59<01:42,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  95% 10219/10702 [35:59<01:42,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10221/10702 [35:59<01:41,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10223/10702 [35:59<01:41,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10225/10702 [35:59<01:40,  4.73it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10227/10702 [35:59<01:40,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10229/10702 [35:59<01:39,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10231/10702 [36:00<01:39,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10233/10702 [36:00<01:39,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10235/10702 [36:00<01:38,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10237/10702 [36:00<01:38,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10239/10702 [36:00<01:37,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10241/10702 [36:00<01:37,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10243/10702 [36:00<01:36,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10245/10702 [36:00<01:36,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10247/10702 [36:01<01:35,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10249/10702 [36:01<01:35,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10251/10702 [36:01<01:35,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10253/10702 [36:01<01:34,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10255/10702 [36:01<01:34,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10257/10702 [36:01<01:33,  4.74it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10259/10702 [36:01<01:33,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10261/10702 [36:02<01:32,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10263/10702 [36:02<01:32,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10265/10702 [36:02<01:32,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10267/10702 [36:02<01:31,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10269/10702 [36:02<01:31,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10271/10702 [36:02<01:30,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10273/10702 [36:02<01:30,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10275/10702 [36:03<01:29,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10277/10702 [36:03<01:29,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10279/10702 [36:03<01:29,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10281/10702 [36:03<01:28,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10283/10702 [36:03<01:28,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10285/10702 [36:03<01:27,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10287/10702 [36:03<01:27,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10289/10702 [36:03<01:26,  4.75it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10291/10702 [36:04<01:26,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10293/10702 [36:04<01:25,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10295/10702 [36:04<01:25,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10297/10702 [36:04<01:25,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10299/10702 [36:04<01:24,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10301/10702 [36:04<01:24,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10303/10702 [36:04<01:23,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10305/10702 [36:05<01:23,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10307/10702 [36:05<01:22,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10309/10702 [36:05<01:22,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10311/10702 [36:05<01:22,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10313/10702 [36:05<01:21,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10315/10702 [36:05<01:21,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10317/10702 [36:05<01:20,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10319/10702 [36:06<01:20,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10321/10702 [36:06<01:19,  4.76it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10323/10702 [36:06<01:19,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10325/10702 [36:06<01:19,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  96% 10327/10702 [36:06<01:18,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10329/10702 [36:06<01:18,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10331/10702 [36:06<01:17,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10333/10702 [36:07<01:17,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10335/10702 [36:07<01:16,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10337/10702 [36:07<01:16,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10339/10702 [36:07<01:16,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10341/10702 [36:07<01:15,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10343/10702 [36:07<01:15,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10345/10702 [36:07<01:14,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10347/10702 [36:07<01:14,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10349/10702 [36:08<01:13,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10351/10702 [36:08<01:13,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10353/10702 [36:08<01:13,  4.77it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10355/10702 [36:08<01:12,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10357/10702 [36:08<01:12,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10359/10702 [36:08<01:11,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10361/10702 [36:08<01:11,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10363/10702 [36:09<01:10,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10365/10702 [36:09<01:10,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10367/10702 [36:09<01:10,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10369/10702 [36:09<01:09,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10371/10702 [36:09<01:09,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10373/10702 [36:09<01:08,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10375/10702 [36:09<01:08,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10377/10702 [36:10<01:07,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10379/10702 [36:10<01:07,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10381/10702 [36:10<01:07,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10383/10702 [36:10<01:06,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10385/10702 [36:10<01:06,  4.78it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10387/10702 [36:10<01:05,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10389/10702 [36:10<01:05,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10391/10702 [36:10<01:04,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10393/10702 [36:11<01:04,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10395/10702 [36:11<01:04,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10397/10702 [36:11<01:03,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10399/10702 [36:11<01:03,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10401/10702 [36:11<01:02,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10403/10702 [36:11<01:02,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10405/10702 [36:11<01:01,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10407/10702 [36:12<01:01,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10409/10702 [36:12<01:01,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10411/10702 [36:12<01:00,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10413/10702 [36:12<01:00,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10415/10702 [36:12<00:59,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10417/10702 [36:12<00:59,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10419/10702 [36:12<00:59,  4.79it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10421/10702 [36:13<00:58,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10423/10702 [36:13<00:58,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10425/10702 [36:13<00:57,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10427/10702 [36:13<00:57,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10429/10702 [36:13<00:56,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10431/10702 [36:13<00:56,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  97% 10433/10702 [36:13<00:56,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10435/10702 [36:13<00:55,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10437/10702 [36:14<00:55,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10439/10702 [36:14<00:54,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10441/10702 [36:14<00:54,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10443/10702 [36:14<00:53,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10445/10702 [36:14<00:53,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10447/10702 [36:14<00:53,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10449/10702 [36:14<00:52,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10451/10702 [36:15<00:52,  4.80it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10453/10702 [36:15<00:51,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10455/10702 [36:15<00:51,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10457/10702 [36:15<00:50,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10459/10702 [36:15<00:50,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10461/10702 [36:15<00:50,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10463/10702 [36:15<00:49,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10465/10702 [36:16<00:49,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10467/10702 [36:16<00:48,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10469/10702 [36:16<00:48,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10471/10702 [36:16<00:48,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10473/10702 [36:16<00:47,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10475/10702 [36:16<00:47,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10477/10702 [36:16<00:46,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10479/10702 [36:16<00:46,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10481/10702 [36:17<00:45,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10483/10702 [36:17<00:45,  4.81it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10485/10702 [36:17<00:45,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10487/10702 [36:17<00:44,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10489/10702 [36:17<00:44,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10491/10702 [36:17<00:43,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10493/10702 [36:17<00:43,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10495/10702 [36:18<00:42,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10497/10702 [36:18<00:42,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10499/10702 [36:18<00:42,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10501/10702 [36:18<00:41,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10503/10702 [36:18<00:41,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10505/10702 [36:18<00:40,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10507/10702 [36:18<00:40,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10509/10702 [36:19<00:40,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10511/10702 [36:19<00:39,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10513/10702 [36:19<00:39,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10515/10702 [36:19<00:38,  4.82it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10517/10702 [36:19<00:38,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10519/10702 [36:19<00:37,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10521/10702 [36:19<00:37,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10523/10702 [36:20<00:37,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10525/10702 [36:20<00:36,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10527/10702 [36:20<00:36,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10529/10702 [36:20<00:35,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10531/10702 [36:20<00:35,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10533/10702 [36:20<00:34,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10535/10702 [36:20<00:34,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10537/10702 [36:20<00:34,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10539/10702 [36:21<00:33,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  98% 10541/10702 [36:21<00:33,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10543/10702 [36:21<00:32,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10545/10702 [36:21<00:32,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10547/10702 [36:21<00:32,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10549/10702 [36:21<00:31,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10551/10702 [36:21<00:31,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10553/10702 [36:22<00:30,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10555/10702 [36:22<00:30,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10557/10702 [36:22<00:29,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10559/10702 [36:22<00:29,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10561/10702 [36:22<00:29,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10563/10702 [36:22<00:28,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10565/10702 [36:22<00:28,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10567/10702 [36:23<00:27,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10569/10702 [36:23<00:27,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10571/10702 [36:23<00:27,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10573/10702 [36:23<00:26,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10575/10702 [36:23<00:26,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10577/10702 [36:23<00:25,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10579/10702 [36:23<00:25,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10581/10702 [36:23<00:24,  4.84it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10583/10702 [36:24<00:24,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10585/10702 [36:24<00:24,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10587/10702 [36:24<00:23,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10589/10702 [36:24<00:23,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10591/10702 [36:24<00:22,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10593/10702 [36:24<00:22,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10595/10702 [36:24<00:22,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10597/10702 [36:25<00:21,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10599/10702 [36:25<00:21,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10601/10702 [36:25<00:20,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10603/10702 [36:25<00:20,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10605/10702 [36:25<00:19,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10607/10702 [36:25<00:19,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10609/10702 [36:25<00:19,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10611/10702 [36:26<00:18,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10613/10702 [36:26<00:18,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10615/10702 [36:26<00:17,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10617/10702 [36:26<00:17,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10619/10702 [36:26<00:17,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10621/10702 [36:26<00:16,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10623/10702 [36:26<00:16,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10625/10702 [36:26<00:15,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10627/10702 [36:27<00:15,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10629/10702 [36:27<00:15,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10631/10702 [36:27<00:14,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10633/10702 [36:27<00:14,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10635/10702 [36:27<00:13,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10637/10702 [36:27<00:13,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10639/10702 [36:27<00:12,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10641/10702 [36:28<00:12,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10643/10702 [36:28<00:12,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10645/10702 [36:28<00:11,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38:  99% 10647/10702 [36:28<00:11,  4.86it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10649/10702 [36:28<00:10,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10651/10702 [36:28<00:10,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10653/10702 [36:28<00:10,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10655/10702 [36:29<00:09,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10657/10702 [36:29<00:09,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10659/10702 [36:29<00:08,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10661/10702 [36:29<00:08,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10663/10702 [36:29<00:08,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10665/10702 [36:29<00:07,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10667/10702 [36:29<00:07,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10669/10702 [36:29<00:06,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10671/10702 [36:30<00:06,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10673/10702 [36:30<00:05,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10675/10702 [36:30<00:05,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10677/10702 [36:30<00:05,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10679/10702 [36:30<00:04,  4.87it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10681/10702 [36:30<00:04,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10683/10702 [36:30<00:03,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10685/10702 [36:31<00:03,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10687/10702 [36:31<00:03,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10689/10702 [36:31<00:02,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10691/10702 [36:31<00:02,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10693/10702 [36:31<00:01,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10695/10702 [36:31<00:01,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10697/10702 [36:31<00:01,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10699/10702 [36:32<00:00,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10701/10702 [36:32<00:00,  4.88it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Validating: 100% 2141/2141 [02:26<00:00, 15.10it/s]\u001b[AEpoch 38, global step 333878: val_loss reached 2.50278 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=38-val_loss=2.503.ckpt\" as top 2\n",
            "INFO:lightning:Epoch 38, global step 333878: val_loss reached 2.50278 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=38-val_loss=2.503.ckpt\" as top 2\n",
            "tcmalloc: large alloc 1651097600 bytes == 0x55ea28558000 @  0x7f036201b615 0x55e896508cdc 0x55e8965e852a 0x55e89650f76c 0x7f03370a1a94 0x7f03370a3864 0x7f0337073590 0x7f03279b9465 0x7f03279b59ca 0x7f03279ba609 0x7f0337076f2b 0x7f0336cfc200 0x55e89650c8a8 0x55e89657ffd5 0x55e89657a7ad 0x55e89650d3ea 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b3b5 0x55e89657a4ae 0x55e89650d3ea 0x55e89657b60e 0x55e89657a4ae 0x55e89650d3ea 0x55e89657f7f0 0x55e89650d30a 0x55e89657b60e 0x55e89650d30a\n",
            "Epoch 38: 100% 10702/10702 [36:48<00:00,  4.85it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.124]\n",
            "Epoch 38: 100% 10702/10702 [36:56<00:00,  4.83it/s, loss=0.0798, v_num=9, val_loss=2.5, train_loss=0.113]\n",
            "Epoch 39:  80% 8561/10702 [34:13<08:33,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]  \n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  80% 8563/10702 [34:13<08:33,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8565/10702 [34:14<08:32,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8567/10702 [34:14<08:31,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8569/10702 [34:14<08:31,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8571/10702 [34:14<08:30,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8573/10702 [34:14<08:30,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8575/10702 [34:14<08:29,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8577/10702 [34:14<08:29,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8579/10702 [34:14<08:28,  4.17it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8581/10702 [34:15<08:27,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8583/10702 [34:15<08:27,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8585/10702 [34:15<08:26,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8587/10702 [34:15<08:26,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8589/10702 [34:15<08:25,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8591/10702 [34:15<08:25,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8593/10702 [34:15<08:24,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8595/10702 [34:16<08:24,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8597/10702 [34:16<08:23,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8599/10702 [34:16<08:22,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8601/10702 [34:16<08:22,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8603/10702 [34:16<08:21,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8605/10702 [34:16<08:21,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8607/10702 [34:16<08:20,  4.18it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8609/10702 [34:17<08:20,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8611/10702 [34:17<08:19,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8613/10702 [34:17<08:18,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  80% 8615/10702 [34:17<08:18,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8617/10702 [34:17<08:17,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8619/10702 [34:17<08:17,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8621/10702 [34:17<08:16,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8623/10702 [34:17<08:16,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8625/10702 [34:18<08:15,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8627/10702 [34:18<08:15,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8629/10702 [34:18<08:14,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8631/10702 [34:18<08:13,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8633/10702 [34:18<08:13,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8635/10702 [34:18<08:12,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8637/10702 [34:18<08:12,  4.19it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8639/10702 [34:19<08:11,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8641/10702 [34:19<08:11,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8643/10702 [34:19<08:10,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8645/10702 [34:19<08:10,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8647/10702 [34:19<08:09,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8649/10702 [34:19<08:08,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8651/10702 [34:19<08:08,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8653/10702 [34:20<08:07,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8655/10702 [34:20<08:07,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8657/10702 [34:20<08:06,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8659/10702 [34:20<08:06,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8661/10702 [34:20<08:05,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8663/10702 [34:20<08:05,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8665/10702 [34:20<08:04,  4.20it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8667/10702 [34:20<08:03,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8669/10702 [34:21<08:03,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8671/10702 [34:21<08:02,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8673/10702 [34:21<08:02,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8675/10702 [34:21<08:01,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8677/10702 [34:21<08:01,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8679/10702 [34:21<08:00,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8681/10702 [34:21<08:00,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8683/10702 [34:22<07:59,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8685/10702 [34:22<07:58,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8687/10702 [34:22<07:58,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8689/10702 [34:22<07:57,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8691/10702 [34:22<07:57,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8693/10702 [34:22<07:56,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8695/10702 [34:22<07:56,  4.21it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8697/10702 [34:23<07:55,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8699/10702 [34:23<07:55,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8701/10702 [34:23<07:54,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8703/10702 [34:23<07:53,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8705/10702 [34:23<07:53,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8707/10702 [34:23<07:52,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8709/10702 [34:23<07:52,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8711/10702 [34:23<07:51,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8713/10702 [34:24<07:51,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8715/10702 [34:24<07:50,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8717/10702 [34:24<07:50,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8719/10702 [34:24<07:49,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  81% 8721/10702 [34:24<07:48,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8723/10702 [34:24<07:48,  4.22it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8725/10702 [34:24<07:47,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8727/10702 [34:25<07:47,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8729/10702 [34:25<07:46,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8731/10702 [34:25<07:46,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8733/10702 [34:25<07:45,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8735/10702 [34:25<07:45,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8737/10702 [34:25<07:44,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8739/10702 [34:25<07:44,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8741/10702 [34:26<07:43,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8743/10702 [34:26<07:42,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8745/10702 [34:26<07:42,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8747/10702 [34:26<07:41,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8749/10702 [34:26<07:41,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8751/10702 [34:26<07:40,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8753/10702 [34:26<07:40,  4.23it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8755/10702 [34:26<07:39,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8757/10702 [34:27<07:39,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8759/10702 [34:27<07:38,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8761/10702 [34:27<07:38,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8763/10702 [34:27<07:37,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8765/10702 [34:27<07:36,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8767/10702 [34:27<07:36,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8769/10702 [34:27<07:35,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8771/10702 [34:28<07:35,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8773/10702 [34:28<07:34,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8775/10702 [34:28<07:34,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8777/10702 [34:28<07:33,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8779/10702 [34:28<07:33,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8781/10702 [34:28<07:32,  4.24it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8783/10702 [34:28<07:32,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8785/10702 [34:29<07:31,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8787/10702 [34:29<07:30,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8789/10702 [34:29<07:30,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8791/10702 [34:29<07:29,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8793/10702 [34:29<07:29,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8795/10702 [34:29<07:28,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8797/10702 [34:29<07:28,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8799/10702 [34:30<07:27,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8801/10702 [34:30<07:27,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8803/10702 [34:30<07:26,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8805/10702 [34:30<07:26,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8807/10702 [34:30<07:25,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8809/10702 [34:30<07:24,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8811/10702 [34:30<07:24,  4.25it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8813/10702 [34:30<07:23,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8815/10702 [34:31<07:23,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8817/10702 [34:31<07:22,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8819/10702 [34:31<07:22,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8821/10702 [34:31<07:21,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8823/10702 [34:31<07:21,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8825/10702 [34:31<07:20,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8827/10702 [34:31<07:20,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  82% 8829/10702 [34:32<07:19,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8831/10702 [34:32<07:19,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8833/10702 [34:32<07:18,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8835/10702 [34:32<07:17,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8837/10702 [34:32<07:17,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8839/10702 [34:32<07:16,  4.26it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8841/10702 [34:32<07:16,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8843/10702 [34:33<07:15,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8845/10702 [34:33<07:15,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8847/10702 [34:33<07:14,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8849/10702 [34:33<07:14,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8851/10702 [34:33<07:13,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8853/10702 [34:33<07:13,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8855/10702 [34:33<07:12,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8857/10702 [34:33<07:12,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8859/10702 [34:34<07:11,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8861/10702 [34:34<07:10,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8863/10702 [34:34<07:10,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8865/10702 [34:34<07:09,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8867/10702 [34:34<07:09,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8869/10702 [34:34<07:08,  4.27it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8871/10702 [34:34<07:08,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8873/10702 [34:35<07:07,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8875/10702 [34:35<07:07,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8877/10702 [34:35<07:06,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8879/10702 [34:35<07:06,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8881/10702 [34:35<07:05,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8883/10702 [34:35<07:05,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8885/10702 [34:35<07:04,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8887/10702 [34:36<07:03,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8889/10702 [34:36<07:03,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8891/10702 [34:36<07:02,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8893/10702 [34:36<07:02,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8895/10702 [34:36<07:01,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8897/10702 [34:36<07:01,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8899/10702 [34:36<07:00,  4.28it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8901/10702 [34:37<07:00,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8903/10702 [34:37<06:59,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8905/10702 [34:37<06:59,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8907/10702 [34:37<06:58,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8909/10702 [34:37<06:58,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8911/10702 [34:37<06:57,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8913/10702 [34:37<06:57,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8915/10702 [34:37<06:56,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8917/10702 [34:38<06:55,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8919/10702 [34:38<06:55,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8921/10702 [34:38<06:54,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8923/10702 [34:38<06:54,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8925/10702 [34:38<06:53,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8927/10702 [34:38<06:53,  4.29it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8929/10702 [34:38<06:52,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8931/10702 [34:39<06:52,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8933/10702 [34:39<06:51,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  83% 8935/10702 [34:39<06:51,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8937/10702 [34:39<06:50,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8939/10702 [34:39<06:50,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8941/10702 [34:39<06:49,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8943/10702 [34:39<06:49,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8945/10702 [34:40<06:48,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8947/10702 [34:40<06:48,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8949/10702 [34:40<06:47,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8951/10702 [34:40<06:46,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8953/10702 [34:40<06:46,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8955/10702 [34:40<06:45,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8957/10702 [34:40<06:45,  4.30it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8959/10702 [34:40<06:44,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8961/10702 [34:41<06:44,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8963/10702 [34:41<06:43,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8965/10702 [34:41<06:43,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8967/10702 [34:41<06:42,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8969/10702 [34:41<06:42,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8971/10702 [34:41<06:41,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8973/10702 [34:41<06:41,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8975/10702 [34:42<06:40,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8977/10702 [34:42<06:40,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8979/10702 [34:42<06:39,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8981/10702 [34:42<06:39,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8983/10702 [34:42<06:38,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8985/10702 [34:42<06:38,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8987/10702 [34:42<06:37,  4.31it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8989/10702 [34:43<06:36,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8991/10702 [34:43<06:36,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8993/10702 [34:43<06:35,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8995/10702 [34:43<06:35,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8997/10702 [34:43<06:34,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 8999/10702 [34:43<06:34,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9001/10702 [34:43<06:33,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9003/10702 [34:43<06:33,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9005/10702 [34:44<06:32,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9007/10702 [34:44<06:32,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9009/10702 [34:44<06:31,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9011/10702 [34:44<06:31,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9013/10702 [34:44<06:30,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9015/10702 [34:44<06:30,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9017/10702 [34:44<06:29,  4.32it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9019/10702 [34:45<06:29,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9021/10702 [34:45<06:28,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9023/10702 [34:45<06:28,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9025/10702 [34:45<06:27,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9027/10702 [34:45<06:26,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9029/10702 [34:45<06:26,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9031/10702 [34:45<06:25,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9033/10702 [34:46<06:25,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9035/10702 [34:46<06:24,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9037/10702 [34:46<06:24,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9039/10702 [34:46<06:23,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9041/10702 [34:46<06:23,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  84% 9043/10702 [34:46<06:22,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9045/10702 [34:46<06:22,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9047/10702 [34:47<06:21,  4.33it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9049/10702 [34:47<06:21,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9051/10702 [34:47<06:20,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9053/10702 [34:47<06:20,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9055/10702 [34:47<06:19,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9057/10702 [34:47<06:19,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9059/10702 [34:47<06:18,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9061/10702 [34:47<06:18,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9063/10702 [34:48<06:17,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9065/10702 [34:48<06:17,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9067/10702 [34:48<06:16,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9069/10702 [34:48<06:16,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9071/10702 [34:48<06:15,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9073/10702 [34:48<06:15,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9075/10702 [34:48<06:14,  4.34it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9077/10702 [34:49<06:13,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9079/10702 [34:49<06:13,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9081/10702 [34:49<06:12,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9083/10702 [34:49<06:12,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9085/10702 [34:49<06:11,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9087/10702 [34:49<06:11,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9089/10702 [34:49<06:10,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9091/10702 [34:50<06:10,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9093/10702 [34:50<06:09,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9095/10702 [34:50<06:09,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9097/10702 [34:50<06:08,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9099/10702 [34:50<06:08,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9101/10702 [34:50<06:07,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9103/10702 [34:50<06:07,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9105/10702 [34:50<06:06,  4.35it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9107/10702 [34:51<06:06,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9109/10702 [34:51<06:05,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9111/10702 [34:51<06:05,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9113/10702 [34:51<06:04,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9115/10702 [34:51<06:04,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9117/10702 [34:51<06:03,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9119/10702 [34:51<06:03,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9121/10702 [34:52<06:02,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9123/10702 [34:52<06:02,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9125/10702 [34:52<06:01,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9127/10702 [34:52<06:01,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9129/10702 [34:52<06:00,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9131/10702 [34:52<06:00,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9133/10702 [34:52<05:59,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9135/10702 [34:53<05:59,  4.36it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9137/10702 [34:53<05:58,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9139/10702 [34:53<05:58,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9141/10702 [34:53<05:57,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9143/10702 [34:53<05:56,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9145/10702 [34:53<05:56,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9147/10702 [34:53<05:55,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  85% 9149/10702 [34:53<05:55,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9151/10702 [34:54<05:54,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9153/10702 [34:54<05:54,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9155/10702 [34:54<05:53,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9157/10702 [34:54<05:53,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9159/10702 [34:54<05:52,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9161/10702 [34:54<05:52,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9163/10702 [34:54<05:51,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9165/10702 [34:55<05:51,  4.37it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9167/10702 [34:55<05:50,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9169/10702 [34:55<05:50,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9171/10702 [34:55<05:49,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9173/10702 [34:55<05:49,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9175/10702 [34:55<05:48,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9177/10702 [34:55<05:48,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9179/10702 [34:56<05:47,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9181/10702 [34:56<05:47,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9183/10702 [34:56<05:46,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9185/10702 [34:56<05:46,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9187/10702 [34:56<05:45,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9189/10702 [34:56<05:45,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9191/10702 [34:56<05:44,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9193/10702 [34:57<05:44,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9195/10702 [34:57<05:43,  4.38it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9197/10702 [34:57<05:43,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9199/10702 [34:57<05:42,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9201/10702 [34:57<05:42,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9203/10702 [34:57<05:41,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9205/10702 [34:57<05:41,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9207/10702 [34:57<05:40,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9209/10702 [34:58<05:40,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9211/10702 [34:58<05:39,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9213/10702 [34:58<05:39,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9215/10702 [34:58<05:38,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9217/10702 [34:58<05:38,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9219/10702 [34:58<05:37,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9221/10702 [34:58<05:37,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9223/10702 [34:59<05:36,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9225/10702 [34:59<05:36,  4.39it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9227/10702 [34:59<05:35,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9229/10702 [34:59<05:35,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9231/10702 [34:59<05:34,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9233/10702 [34:59<05:34,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9235/10702 [34:59<05:33,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9237/10702 [35:00<05:33,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9239/10702 [35:00<05:32,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9241/10702 [35:00<05:32,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9243/10702 [35:00<05:31,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9245/10702 [35:00<05:31,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9247/10702 [35:00<05:30,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9249/10702 [35:00<05:30,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9251/10702 [35:00<05:29,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9253/10702 [35:01<05:29,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9255/10702 [35:01<05:28,  4.40it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  86% 9257/10702 [35:01<05:28,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9259/10702 [35:01<05:27,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9261/10702 [35:01<05:27,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9263/10702 [35:01<05:26,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9265/10702 [35:01<05:26,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9267/10702 [35:02<05:25,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9269/10702 [35:02<05:25,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9271/10702 [35:02<05:24,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9273/10702 [35:02<05:23,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9275/10702 [35:02<05:23,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9277/10702 [35:02<05:22,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9279/10702 [35:02<05:22,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9281/10702 [35:03<05:21,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9283/10702 [35:03<05:21,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9285/10702 [35:03<05:20,  4.41it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9287/10702 [35:03<05:20,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9289/10702 [35:03<05:19,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9291/10702 [35:03<05:19,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9293/10702 [35:03<05:18,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9295/10702 [35:03<05:18,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9297/10702 [35:04<05:17,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9299/10702 [35:04<05:17,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9301/10702 [35:04<05:16,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9303/10702 [35:04<05:16,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9305/10702 [35:04<05:15,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9307/10702 [35:04<05:15,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9309/10702 [35:04<05:14,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9311/10702 [35:05<05:14,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9313/10702 [35:05<05:13,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9315/10702 [35:05<05:13,  4.42it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9317/10702 [35:05<05:12,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9319/10702 [35:05<05:12,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9321/10702 [35:05<05:11,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9323/10702 [35:05<05:11,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9325/10702 [35:06<05:10,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9327/10702 [35:06<05:10,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9329/10702 [35:06<05:09,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9331/10702 [35:06<05:09,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9333/10702 [35:06<05:09,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9335/10702 [35:06<05:08,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9337/10702 [35:06<05:08,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9339/10702 [35:06<05:07,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9341/10702 [35:07<05:07,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9343/10702 [35:07<05:06,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9345/10702 [35:07<05:06,  4.43it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9347/10702 [35:07<05:05,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9349/10702 [35:07<05:05,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9351/10702 [35:07<05:04,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9353/10702 [35:07<05:04,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9355/10702 [35:08<05:03,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9357/10702 [35:08<05:03,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9359/10702 [35:08<05:02,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9361/10702 [35:08<05:02,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  87% 9363/10702 [35:08<05:01,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9365/10702 [35:08<05:01,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9367/10702 [35:08<05:00,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9369/10702 [35:09<05:00,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9371/10702 [35:09<04:59,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9373/10702 [35:09<04:59,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9375/10702 [35:09<04:58,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9377/10702 [35:09<04:58,  4.44it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9379/10702 [35:09<04:57,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9381/10702 [35:09<04:57,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9383/10702 [35:10<04:56,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9385/10702 [35:10<04:56,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9387/10702 [35:10<04:55,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9389/10702 [35:10<04:55,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9391/10702 [35:10<04:54,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9393/10702 [35:10<04:54,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9395/10702 [35:10<04:53,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9397/10702 [35:10<04:53,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9399/10702 [35:11<04:52,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9401/10702 [35:11<04:52,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9403/10702 [35:11<04:51,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9405/10702 [35:11<04:51,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9407/10702 [35:11<04:50,  4.45it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9409/10702 [35:11<04:50,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9411/10702 [35:11<04:49,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9413/10702 [35:12<04:49,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9415/10702 [35:12<04:48,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9417/10702 [35:12<04:48,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9419/10702 [35:12<04:47,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9421/10702 [35:12<04:47,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9423/10702 [35:12<04:46,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9425/10702 [35:12<04:46,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9427/10702 [35:13<04:45,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9429/10702 [35:13<04:45,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9431/10702 [35:13<04:44,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9433/10702 [35:13<04:44,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9435/10702 [35:13<04:43,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9437/10702 [35:13<04:43,  4.46it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9439/10702 [35:13<04:42,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9441/10702 [35:13<04:42,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9443/10702 [35:14<04:41,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9445/10702 [35:14<04:41,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9447/10702 [35:14<04:40,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9449/10702 [35:14<04:40,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9451/10702 [35:14<04:39,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9453/10702 [35:14<04:39,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9455/10702 [35:14<04:38,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9457/10702 [35:15<04:38,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9459/10702 [35:15<04:37,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9461/10702 [35:15<04:37,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9463/10702 [35:15<04:36,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9465/10702 [35:15<04:36,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9467/10702 [35:15<04:36,  4.47it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9469/10702 [35:15<04:35,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  88% 9471/10702 [35:16<04:35,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9473/10702 [35:16<04:34,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9475/10702 [35:16<04:34,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9477/10702 [35:16<04:33,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9479/10702 [35:16<04:33,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9481/10702 [35:16<04:32,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9483/10702 [35:16<04:32,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9485/10702 [35:17<04:31,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9487/10702 [35:17<04:31,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9489/10702 [35:17<04:30,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9491/10702 [35:17<04:30,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9493/10702 [35:17<04:29,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9495/10702 [35:17<04:29,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9497/10702 [35:17<04:28,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9499/10702 [35:17<04:28,  4.48it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9501/10702 [35:18<04:27,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9503/10702 [35:18<04:27,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9505/10702 [35:18<04:26,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9507/10702 [35:18<04:26,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9509/10702 [35:18<04:25,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9511/10702 [35:18<04:25,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9513/10702 [35:18<04:24,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9515/10702 [35:19<04:24,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9517/10702 [35:19<04:23,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9519/10702 [35:19<04:23,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9521/10702 [35:19<04:22,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9523/10702 [35:19<04:22,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9525/10702 [35:19<04:21,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9527/10702 [35:19<04:21,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9529/10702 [35:20<04:20,  4.49it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9531/10702 [35:20<04:20,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9533/10702 [35:20<04:20,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9535/10702 [35:20<04:19,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9537/10702 [35:20<04:19,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9539/10702 [35:20<04:18,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9541/10702 [35:20<04:18,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9543/10702 [35:20<04:17,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9545/10702 [35:21<04:17,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9547/10702 [35:21<04:16,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9549/10702 [35:21<04:16,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9551/10702 [35:21<04:15,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9553/10702 [35:21<04:15,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9555/10702 [35:21<04:14,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9557/10702 [35:21<04:14,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9559/10702 [35:22<04:13,  4.50it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9561/10702 [35:22<04:13,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9563/10702 [35:22<04:12,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9565/10702 [35:22<04:12,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9567/10702 [35:22<04:11,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9569/10702 [35:22<04:11,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9571/10702 [35:22<04:10,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9573/10702 [35:23<04:10,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9575/10702 [35:23<04:09,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  89% 9577/10702 [35:23<04:09,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9579/10702 [35:23<04:08,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9581/10702 [35:23<04:08,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9583/10702 [35:23<04:07,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9585/10702 [35:23<04:07,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9587/10702 [35:23<04:07,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9589/10702 [35:24<04:06,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9591/10702 [35:24<04:06,  4.51it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9593/10702 [35:24<04:05,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9595/10702 [35:24<04:05,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9597/10702 [35:24<04:04,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9599/10702 [35:24<04:04,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9601/10702 [35:24<04:03,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9603/10702 [35:25<04:03,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9605/10702 [35:25<04:02,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9607/10702 [35:25<04:02,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9609/10702 [35:25<04:01,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9611/10702 [35:25<04:01,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9613/10702 [35:25<04:00,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9615/10702 [35:25<04:00,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9617/10702 [35:26<03:59,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9619/10702 [35:26<03:59,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9621/10702 [35:26<03:58,  4.52it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9623/10702 [35:26<03:58,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9625/10702 [35:26<03:57,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9627/10702 [35:26<03:57,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9629/10702 [35:26<03:57,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9631/10702 [35:27<03:56,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9633/10702 [35:27<03:56,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9635/10702 [35:27<03:55,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9637/10702 [35:27<03:55,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9639/10702 [35:27<03:54,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9641/10702 [35:27<03:54,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9643/10702 [35:27<03:53,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9645/10702 [35:27<03:53,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9647/10702 [35:28<03:52,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9649/10702 [35:28<03:52,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9651/10702 [35:28<03:51,  4.53it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9653/10702 [35:28<03:51,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9655/10702 [35:28<03:50,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9657/10702 [35:28<03:50,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9659/10702 [35:28<03:49,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9661/10702 [35:29<03:49,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9663/10702 [35:29<03:48,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9665/10702 [35:29<03:48,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9667/10702 [35:29<03:47,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9669/10702 [35:29<03:47,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9671/10702 [35:29<03:47,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9673/10702 [35:29<03:46,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9675/10702 [35:30<03:46,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9677/10702 [35:30<03:45,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9679/10702 [35:30<03:45,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9681/10702 [35:30<03:44,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9683/10702 [35:30<03:44,  4.54it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  90% 9685/10702 [35:30<03:43,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9687/10702 [35:30<03:43,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9689/10702 [35:30<03:42,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9691/10702 [35:31<03:42,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9693/10702 [35:31<03:41,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9695/10702 [35:31<03:41,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9697/10702 [35:31<03:40,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9699/10702 [35:31<03:40,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9701/10702 [35:31<03:39,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9703/10702 [35:31<03:39,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9705/10702 [35:32<03:39,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9707/10702 [35:32<03:38,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9709/10702 [35:32<03:38,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9711/10702 [35:32<03:37,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9713/10702 [35:32<03:37,  4.55it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9715/10702 [35:32<03:36,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9717/10702 [35:32<03:36,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9719/10702 [35:33<03:35,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9721/10702 [35:33<03:35,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9723/10702 [35:33<03:34,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9725/10702 [35:33<03:34,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9727/10702 [35:33<03:33,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9729/10702 [35:33<03:33,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9731/10702 [35:33<03:32,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9733/10702 [35:34<03:32,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9735/10702 [35:34<03:31,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9737/10702 [35:34<03:31,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9739/10702 [35:34<03:31,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9741/10702 [35:34<03:30,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9743/10702 [35:34<03:30,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9745/10702 [35:34<03:29,  4.56it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9747/10702 [35:34<03:29,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9749/10702 [35:35<03:28,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9751/10702 [35:35<03:28,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9753/10702 [35:35<03:27,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9755/10702 [35:35<03:27,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9757/10702 [35:35<03:26,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9759/10702 [35:35<03:26,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9761/10702 [35:35<03:25,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9763/10702 [35:36<03:25,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9765/10702 [35:36<03:24,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9767/10702 [35:36<03:24,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9769/10702 [35:36<03:24,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9771/10702 [35:36<03:23,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9773/10702 [35:36<03:23,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9775/10702 [35:36<03:22,  4.57it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9777/10702 [35:37<03:22,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9779/10702 [35:37<03:21,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9781/10702 [35:37<03:21,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9783/10702 [35:37<03:20,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9785/10702 [35:37<03:20,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9787/10702 [35:37<03:19,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9789/10702 [35:37<03:19,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  91% 9791/10702 [35:37<03:18,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9793/10702 [35:38<03:18,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9795/10702 [35:38<03:17,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9797/10702 [35:38<03:17,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9799/10702 [35:38<03:17,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9801/10702 [35:38<03:16,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9803/10702 [35:38<03:16,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9805/10702 [35:38<03:15,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9807/10702 [35:39<03:15,  4.58it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9809/10702 [35:39<03:14,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9811/10702 [35:39<03:14,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9813/10702 [35:39<03:13,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9815/10702 [35:39<03:13,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9817/10702 [35:39<03:12,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9819/10702 [35:39<03:12,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9821/10702 [35:40<03:11,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9823/10702 [35:40<03:11,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9825/10702 [35:40<03:11,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9827/10702 [35:40<03:10,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9829/10702 [35:40<03:10,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9831/10702 [35:40<03:09,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9833/10702 [35:40<03:09,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9835/10702 [35:41<03:08,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9837/10702 [35:41<03:08,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9839/10702 [35:41<03:07,  4.59it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9841/10702 [35:41<03:07,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9843/10702 [35:41<03:06,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9845/10702 [35:41<03:06,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9847/10702 [35:41<03:05,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9849/10702 [35:41<03:05,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9851/10702 [35:42<03:05,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9853/10702 [35:42<03:04,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9855/10702 [35:42<03:04,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9857/10702 [35:42<03:03,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9859/10702 [35:42<03:03,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9861/10702 [35:42<03:02,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9863/10702 [35:42<03:02,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9865/10702 [35:43<03:01,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9867/10702 [35:43<03:01,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9869/10702 [35:43<03:00,  4.60it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9871/10702 [35:43<03:00,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9873/10702 [35:43<02:59,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9875/10702 [35:43<02:59,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9877/10702 [35:43<02:59,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9879/10702 [35:44<02:58,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9881/10702 [35:44<02:58,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9883/10702 [35:44<02:57,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9885/10702 [35:44<02:57,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9887/10702 [35:44<02:56,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9889/10702 [35:44<02:56,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9891/10702 [35:44<02:55,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9893/10702 [35:44<02:55,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9895/10702 [35:45<02:54,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9897/10702 [35:45<02:54,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  92% 9899/10702 [35:45<02:54,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9901/10702 [35:45<02:53,  4.61it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9903/10702 [35:45<02:53,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9905/10702 [35:45<02:52,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9907/10702 [35:45<02:52,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9909/10702 [35:46<02:51,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9911/10702 [35:46<02:51,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9913/10702 [35:46<02:50,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9915/10702 [35:46<02:50,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9917/10702 [35:46<02:49,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9919/10702 [35:46<02:49,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9921/10702 [35:46<02:49,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9923/10702 [35:47<02:48,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9925/10702 [35:47<02:48,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9927/10702 [35:47<02:47,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9929/10702 [35:47<02:47,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9931/10702 [35:47<02:46,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9933/10702 [35:47<02:46,  4.62it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9935/10702 [35:47<02:45,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9937/10702 [35:48<02:45,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9939/10702 [35:48<02:44,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9941/10702 [35:48<02:44,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9943/10702 [35:48<02:43,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9945/10702 [35:48<02:43,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9947/10702 [35:48<02:43,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9949/10702 [35:48<02:42,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9951/10702 [35:48<02:42,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9953/10702 [35:49<02:41,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9955/10702 [35:49<02:41,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9957/10702 [35:49<02:40,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9959/10702 [35:49<02:40,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9961/10702 [35:49<02:39,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9963/10702 [35:49<02:39,  4.63it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9965/10702 [35:49<02:39,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9967/10702 [35:50<02:38,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9969/10702 [35:50<02:38,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9971/10702 [35:50<02:37,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9973/10702 [35:50<02:37,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9975/10702 [35:50<02:36,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9977/10702 [35:50<02:36,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9979/10702 [35:50<02:35,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9981/10702 [35:51<02:35,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9983/10702 [35:51<02:34,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9985/10702 [35:51<02:34,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9987/10702 [35:51<02:34,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9989/10702 [35:51<02:33,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9991/10702 [35:51<02:33,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9993/10702 [35:51<02:32,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9995/10702 [35:51<02:32,  4.64it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9997/10702 [35:52<02:31,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 9999/10702 [35:52<02:31,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 10001/10702 [35:52<02:30,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 10003/10702 [35:52<02:30,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  93% 10005/10702 [35:52<02:29,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10007/10702 [35:52<02:29,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10009/10702 [35:52<02:29,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10011/10702 [35:53<02:28,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10013/10702 [35:53<02:28,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10015/10702 [35:53<02:27,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10017/10702 [35:53<02:27,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10019/10702 [35:53<02:26,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10021/10702 [35:53<02:26,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10023/10702 [35:53<02:25,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10025/10702 [35:54<02:25,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10027/10702 [35:54<02:25,  4.65it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10029/10702 [35:54<02:24,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10031/10702 [35:54<02:24,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10033/10702 [35:54<02:23,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10035/10702 [35:54<02:23,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10037/10702 [35:54<02:22,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10039/10702 [35:55<02:22,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10041/10702 [35:55<02:21,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10043/10702 [35:55<02:21,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10045/10702 [35:55<02:20,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10047/10702 [35:55<02:20,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10049/10702 [35:55<02:20,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10051/10702 [35:55<02:19,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10053/10702 [35:55<02:19,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10055/10702 [35:56<02:18,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10057/10702 [35:56<02:18,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10059/10702 [35:56<02:17,  4.66it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10061/10702 [35:56<02:17,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10063/10702 [35:56<02:16,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10065/10702 [35:56<02:16,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10067/10702 [35:56<02:16,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10069/10702 [35:57<02:15,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10071/10702 [35:57<02:15,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10073/10702 [35:57<02:14,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10075/10702 [35:57<02:14,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10077/10702 [35:57<02:13,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10079/10702 [35:57<02:13,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10081/10702 [35:57<02:12,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10083/10702 [35:58<02:12,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10085/10702 [35:58<02:12,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10087/10702 [35:58<02:11,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10089/10702 [35:58<02:11,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10091/10702 [35:58<02:10,  4.67it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10093/10702 [35:58<02:10,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10095/10702 [35:58<02:09,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10097/10702 [35:58<02:09,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10099/10702 [35:59<02:08,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10101/10702 [35:59<02:08,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10103/10702 [35:59<02:08,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10105/10702 [35:59<02:07,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10107/10702 [35:59<02:07,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10109/10702 [35:59<02:06,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10111/10702 [35:59<02:06,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  94% 10113/10702 [36:00<02:05,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10115/10702 [36:00<02:05,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10117/10702 [36:00<02:04,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10119/10702 [36:00<02:04,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10121/10702 [36:00<02:04,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10123/10702 [36:00<02:03,  4.68it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10125/10702 [36:00<02:03,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10127/10702 [36:01<02:02,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10129/10702 [36:01<02:02,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10131/10702 [36:01<02:01,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10133/10702 [36:01<02:01,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10135/10702 [36:01<02:00,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10137/10702 [36:01<02:00,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10139/10702 [36:01<02:00,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10141/10702 [36:01<01:59,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10143/10702 [36:02<01:59,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10145/10702 [36:02<01:58,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10147/10702 [36:02<01:58,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10149/10702 [36:02<01:57,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10151/10702 [36:02<01:57,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10153/10702 [36:02<01:56,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10155/10702 [36:02<01:56,  4.69it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10157/10702 [36:03<01:56,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10159/10702 [36:03<01:55,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10161/10702 [36:03<01:55,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10163/10702 [36:03<01:54,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10165/10702 [36:03<01:54,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10167/10702 [36:03<01:53,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10169/10702 [36:03<01:53,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10171/10702 [36:04<01:52,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10173/10702 [36:04<01:52,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10175/10702 [36:04<01:52,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10177/10702 [36:04<01:51,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10179/10702 [36:04<01:51,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10181/10702 [36:04<01:50,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10183/10702 [36:04<01:50,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10185/10702 [36:05<01:49,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10187/10702 [36:05<01:49,  4.70it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10189/10702 [36:05<01:49,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10191/10702 [36:05<01:48,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10193/10702 [36:05<01:48,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10195/10702 [36:05<01:47,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10197/10702 [36:05<01:47,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10199/10702 [36:05<01:46,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10201/10702 [36:06<01:46,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10203/10702 [36:06<01:45,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10205/10702 [36:06<01:45,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10207/10702 [36:06<01:45,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10209/10702 [36:06<01:44,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10211/10702 [36:06<01:44,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10213/10702 [36:06<01:43,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10215/10702 [36:07<01:43,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10217/10702 [36:07<01:42,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  95% 10219/10702 [36:07<01:42,  4.71it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10221/10702 [36:07<01:42,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10223/10702 [36:07<01:41,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10225/10702 [36:07<01:41,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10227/10702 [36:07<01:40,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10229/10702 [36:08<01:40,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10231/10702 [36:08<01:39,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10233/10702 [36:08<01:39,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10235/10702 [36:08<01:38,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10237/10702 [36:08<01:38,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10239/10702 [36:08<01:38,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10241/10702 [36:08<01:37,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10243/10702 [36:08<01:37,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10245/10702 [36:09<01:36,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10247/10702 [36:09<01:36,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10249/10702 [36:09<01:35,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10251/10702 [36:09<01:35,  4.72it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10253/10702 [36:09<01:35,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10255/10702 [36:09<01:34,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10257/10702 [36:09<01:34,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10259/10702 [36:10<01:33,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10261/10702 [36:10<01:33,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10263/10702 [36:10<01:32,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10265/10702 [36:10<01:32,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10267/10702 [36:10<01:31,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10269/10702 [36:10<01:31,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10271/10702 [36:10<01:31,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10273/10702 [36:11<01:30,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10275/10702 [36:11<01:30,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10277/10702 [36:11<01:29,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10279/10702 [36:11<01:29,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10281/10702 [36:11<01:28,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10283/10702 [36:11<01:28,  4.73it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10285/10702 [36:11<01:28,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10287/10702 [36:12<01:27,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10289/10702 [36:12<01:27,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10291/10702 [36:12<01:26,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10293/10702 [36:12<01:26,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10295/10702 [36:12<01:25,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10297/10702 [36:12<01:25,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10299/10702 [36:12<01:25,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10301/10702 [36:12<01:24,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10303/10702 [36:13<01:24,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10305/10702 [36:13<01:23,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10307/10702 [36:13<01:23,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10309/10702 [36:13<01:22,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10311/10702 [36:13<01:22,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10313/10702 [36:13<01:21,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10315/10702 [36:13<01:21,  4.74it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10317/10702 [36:14<01:21,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10319/10702 [36:14<01:20,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10321/10702 [36:14<01:20,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10323/10702 [36:14<01:19,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10325/10702 [36:14<01:19,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  96% 10327/10702 [36:14<01:18,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10329/10702 [36:14<01:18,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10331/10702 [36:15<01:18,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10333/10702 [36:15<01:17,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10335/10702 [36:15<01:17,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10337/10702 [36:15<01:16,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10339/10702 [36:15<01:16,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10341/10702 [36:15<01:15,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10343/10702 [36:15<01:15,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10345/10702 [36:15<01:15,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10347/10702 [36:16<01:14,  4.75it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10349/10702 [36:16<01:14,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10351/10702 [36:16<01:13,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10353/10702 [36:16<01:13,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10355/10702 [36:16<01:12,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10357/10702 [36:16<01:12,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10359/10702 [36:16<01:12,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10361/10702 [36:17<01:11,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10363/10702 [36:17<01:11,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10365/10702 [36:17<01:10,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10367/10702 [36:17<01:10,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10369/10702 [36:17<01:09,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10371/10702 [36:17<01:09,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10373/10702 [36:17<01:09,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10375/10702 [36:18<01:08,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10377/10702 [36:18<01:08,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10379/10702 [36:18<01:07,  4.76it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10381/10702 [36:18<01:07,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10383/10702 [36:18<01:06,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10385/10702 [36:18<01:06,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10387/10702 [36:18<01:06,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10389/10702 [36:19<01:05,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10391/10702 [36:19<01:05,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10393/10702 [36:19<01:04,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10395/10702 [36:19<01:04,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10397/10702 [36:19<01:03,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10399/10702 [36:19<01:03,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10401/10702 [36:19<01:03,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10403/10702 [36:19<01:02,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10405/10702 [36:20<01:02,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10407/10702 [36:20<01:01,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10409/10702 [36:20<01:01,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10411/10702 [36:20<01:00,  4.77it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10413/10702 [36:20<01:00,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10415/10702 [36:20<01:00,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10417/10702 [36:20<00:59,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10419/10702 [36:21<00:59,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10421/10702 [36:21<00:58,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10423/10702 [36:21<00:58,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10425/10702 [36:21<00:57,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10427/10702 [36:21<00:57,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10429/10702 [36:21<00:57,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10431/10702 [36:21<00:56,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  97% 10433/10702 [36:22<00:56,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10435/10702 [36:22<00:55,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10437/10702 [36:22<00:55,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10439/10702 [36:22<00:54,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10441/10702 [36:22<00:54,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10443/10702 [36:22<00:54,  4.78it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10445/10702 [36:22<00:53,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10447/10702 [36:22<00:53,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10449/10702 [36:23<00:52,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10451/10702 [36:23<00:52,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10453/10702 [36:23<00:52,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10455/10702 [36:23<00:51,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10457/10702 [36:23<00:51,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10459/10702 [36:23<00:50,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10461/10702 [36:23<00:50,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10463/10702 [36:24<00:49,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10465/10702 [36:24<00:49,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10467/10702 [36:24<00:49,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10469/10702 [36:24<00:48,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10471/10702 [36:24<00:48,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10473/10702 [36:24<00:47,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10475/10702 [36:24<00:47,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10477/10702 [36:25<00:46,  4.79it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10479/10702 [36:25<00:46,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10481/10702 [36:25<00:46,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10483/10702 [36:25<00:45,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10485/10702 [36:25<00:45,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10487/10702 [36:25<00:44,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10489/10702 [36:25<00:44,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10491/10702 [36:26<00:43,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10493/10702 [36:26<00:43,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10495/10702 [36:26<00:43,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10497/10702 [36:26<00:42,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10499/10702 [36:26<00:42,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10501/10702 [36:26<00:41,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10503/10702 [36:26<00:41,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10505/10702 [36:26<00:41,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10507/10702 [36:27<00:40,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10509/10702 [36:27<00:40,  4.80it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10511/10702 [36:27<00:39,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10513/10702 [36:27<00:39,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10515/10702 [36:27<00:38,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10517/10702 [36:27<00:38,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10519/10702 [36:27<00:38,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10521/10702 [36:28<00:37,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10523/10702 [36:28<00:37,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10525/10702 [36:28<00:36,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10527/10702 [36:28<00:36,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10529/10702 [36:28<00:35,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10531/10702 [36:28<00:35,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10533/10702 [36:28<00:35,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10535/10702 [36:29<00:34,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10537/10702 [36:29<00:34,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10539/10702 [36:29<00:33,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  98% 10541/10702 [36:29<00:33,  4.81it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10543/10702 [36:29<00:33,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10545/10702 [36:29<00:32,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10547/10702 [36:29<00:32,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10549/10702 [36:29<00:31,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10551/10702 [36:30<00:31,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10553/10702 [36:30<00:30,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10555/10702 [36:30<00:30,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10557/10702 [36:30<00:30,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10559/10702 [36:30<00:29,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10561/10702 [36:30<00:29,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10563/10702 [36:30<00:28,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10565/10702 [36:31<00:28,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10567/10702 [36:31<00:27,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10569/10702 [36:31<00:27,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10571/10702 [36:31<00:27,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10573/10702 [36:31<00:26,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10575/10702 [36:31<00:26,  4.82it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10577/10702 [36:31<00:25,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10579/10702 [36:32<00:25,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10581/10702 [36:32<00:25,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10583/10702 [36:32<00:24,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10585/10702 [36:32<00:24,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10587/10702 [36:32<00:23,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10589/10702 [36:32<00:23,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10591/10702 [36:32<00:22,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10593/10702 [36:33<00:22,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10595/10702 [36:33<00:22,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10597/10702 [36:33<00:21,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10599/10702 [36:33<00:21,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10601/10702 [36:33<00:20,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10603/10702 [36:33<00:20,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10605/10702 [36:33<00:20,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10607/10702 [36:33<00:19,  4.83it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10609/10702 [36:34<00:19,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10611/10702 [36:34<00:18,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10613/10702 [36:34<00:18,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10615/10702 [36:34<00:17,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10617/10702 [36:34<00:17,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10619/10702 [36:34<00:17,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10621/10702 [36:34<00:16,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10623/10702 [36:35<00:16,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10625/10702 [36:35<00:15,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10627/10702 [36:35<00:15,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10629/10702 [36:35<00:15,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10631/10702 [36:35<00:14,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10633/10702 [36:35<00:14,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10635/10702 [36:35<00:13,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10637/10702 [36:36<00:13,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10639/10702 [36:36<00:13,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10641/10702 [36:36<00:12,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10643/10702 [36:36<00:12,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10645/10702 [36:36<00:11,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39:  99% 10647/10702 [36:36<00:11,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10649/10702 [36:36<00:10,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10651/10702 [36:36<00:10,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10653/10702 [36:37<00:10,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10655/10702 [36:37<00:09,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10657/10702 [36:37<00:09,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10659/10702 [36:37<00:08,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10661/10702 [36:37<00:08,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10663/10702 [36:37<00:08,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10665/10702 [36:37<00:07,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10667/10702 [36:38<00:07,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10669/10702 [36:38<00:06,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10671/10702 [36:38<00:06,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10673/10702 [36:38<00:05,  4.85it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10675/10702 [36:38<00:05,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10677/10702 [36:38<00:05,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10679/10702 [36:38<00:04,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10681/10702 [36:39<00:04,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10683/10702 [36:39<00:03,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10685/10702 [36:39<00:03,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10687/10702 [36:39<00:03,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10689/10702 [36:39<00:02,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10691/10702 [36:39<00:02,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10693/10702 [36:39<00:01,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10695/10702 [36:40<00:01,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10697/10702 [36:40<00:01,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10699/10702 [36:40<00:00,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10701/10702 [36:40<00:00,  4.86it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Validating: 100% 2141/2141 [02:26<00:00, 15.02it/s]\u001b[AEpoch 39, global step 342439: val_loss reached 2.55037 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=39-val_loss=2.550.ckpt\" as top 3\n",
            "INFO:lightning:Epoch 39, global step 342439: val_loss reached 2.55037 (best 2.49694), saving model to \"/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=39-val_loss=2.550.ckpt\" as top 3\n",
            "Epoch 39: 100% 10702/10702 [36:51<00:00,  4.84it/s, loss=0.06, v_num=9, val_loss=2.5, train_loss=0.0459]\n",
            "Epoch 39: 100% 10702/10702 [37:03<00:00,  4.81it/s, loss=0.06, v_num=9, val_loss=2.55, train_loss=0.032]\n",
            "                                                   \u001b[ASaving latest checkpoint...\n",
            "INFO:lightning:Saving latest checkpoint...\n",
            "Epoch 39: 100% 10702/10702 [37:04<00:00,  4.81it/s, loss=0.06, v_num=9, val_loss=2.55, train_loss=0.032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWIO_It-Ipn"
      },
      "source": [
        "## binary 파일로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM7hGuwp1sxv",
        "outputId": "f95bc45e-1da1-408d-c125-9e0c5b65a1b8"
      },
      "source": [
        "!python get_model_binary.py --hparams '/content/drive/MyDrive/kobart/logs/tb_logs/default/version_1/hparams.yaml' --model_binary '/content/drive/MyDrive/kobart/logs/kobart_summary-model_chp/epoch=02-val_loss=1.339.ckpt'\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  hparams = yaml.load(f)\n",
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFyfZ3OP-KBp"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpQGol58xqJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c89e1c5-488d-4d85-94d1-9fa79e57d6df"
      },
      "source": [
        "import torch\n",
        "from kobart import get_kobart_tokenizer\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "def load_model():\n",
        "    model = BartForConditionalGeneration.from_pretrained('./kobart_summary')\n",
        "    # tokenizer = get_kobart_tokenizer()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "tokenizer = get_kobart_tokenizer()\n",
        "\n",
        "text = input()\n",
        "\n",
        "if text:\n",
        "    text = text.replace('\\n', '')\n",
        "    print(\"## KoBART 요약 결과\")\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    output = model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "부동산 투자로 억대의 돈을 번 상도는 최근 N×N 크기의 땅을 구매했다. 상도는 손쉬운 땅 관리를 위해 땅을 1×1 크기의 칸으로 나누어 놓았다. 각각의 칸은 (r, c)로 나타내며, r은 가장 위에서부터 떨어진 칸의 개수, c는 가장 왼쪽으로부터 떨어진 칸의 개수이다. r과 c는 1부터 시작한다.상도는 전자통신공학과 출신답게 땅의 양분을 조사하는 로봇 S2D2를 만들었다. S2D2는 1×1 크기의 칸에 들어있는 양분을 조사해 상도에게 전송하고, 모든 칸에 대해서 조사를 한다. 가장 처음에 양분은 모든 칸에 5만큼 들어있다.매일 매일 넓은 땅을 보면서 뿌듯한 하루를 보내고 있던 어느 날 이런 생각이 들었다.나무 재테크를 하자!나무 재테크란 작은 묘목을 구매해 어느정도 키운 후 팔아서 수익을 얻는 재테크이다. 상도는 나무 재테크로 더 큰 돈을 벌기 위해 M개의 나무를 구매해 땅에 심었다. 같은 1×1 크기의 칸에 여러 개의 나무가 심어져 있을 수도 있다.이 나무는 사계절을 보내며, 아래와 같은 과정을 반복한다.봄에는 나무가 자신의 나이만큼 양분을 먹고, 나이가 1 증가한다. 각각의 나무는 나무가 있는 1×1 크기의 칸에 있는 양분만 먹을 수 있다. 하나의 칸에 여러 개의 나무가 있다면, 나이가 어린 나무부터 양분을 먹는다. 만약, 땅에 양분이 부족해 자신의 나이만큼 양분을 먹을 수 없는 나무는 양분을 먹지 못하고 즉시 죽는다.여름에는 봄에 죽은 나무가 양분으로 변하게 된다. 각각의 죽은 나무마다 나이를 2로 나눈 값이 나무가 있던 칸에 양분으로 추가된다. 소수점 아래는 버린다.가을에는 나무가 번식한다. 번식하는 나무는 나이가 5의 배수이어야 하며, 인접한 8개의 칸에 나이가 1인 나무가 생긴다. 어떤 칸 (r, c)와 인접한 칸은 (r-1, c-1), (r-1, c), (r-1, c+1), (r, c-1), (r, c+1), (r+1, c-1), (r+1, c), (r+1, c+1) 이다. 상도의 땅을 벗어나는 칸에는 나무가 생기지 않는다.겨울에는 S2D2가 땅을 돌아다니면서 땅에 양분을 추가한다. 각 칸에 추가되는 양분의 양은 A[r][c]이고, 입력으로 주어진다.K년이 지난 후 상도의 땅에 살아있는 나무의 개수를 구하는 프로그램을 작성하시오.\n",
            "## KoBART 요약 결과\n",
            "전자통신공학과 출신답게 땅의 양분을 조사하는 로봇 S2D2를 만들어 땅의 양분을 조사하는 로봇 S2D2를 만든 상도는 최근 N×N 크기의 땅을 구매해 땅의 양분을 조사해 상도에게 전송하고, 모든 칸에 대해서 조사를 하는 등 손쉬운 땅 관리를 위해 땅을 1×1 크기의 칸으로 나누어 놓았다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvBaTPX6zfkN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}