{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ps_classifications_gluon_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgHJ8VdFZz0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df28ee3f-ccb8-4761-e142-d69509a72740"
      },
      "source": [
        "from google.colab import drive\n",
        "#import os, sys\n",
        "drive.mount('/content/drive', force_remount= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvjqOdDgT5zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481b90bf-afb9-4cc6-8789-0a4a4b60bed8"
      },
      "source": [
        "!pip install mxnet-cu101mkl\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101mkl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/3f/e33e3f92110fa5caba5e9eb052008208a33c1d5faccc7fe5312532e9aa42/mxnet_cu101mkl-1.6.0.post0-py2.py3-none-manylinux1_x86_64.whl (712.3MB)\n",
            "\u001b[K     |████████████████████████████████| 712.3MB 25kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101mkl) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101mkl) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (2.10)\n",
            "Installing collected packages: graphviz, mxnet-cu101mkl\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101mkl-1.6.0.post0\n",
            "Collecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (20.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595679 sha256=cba9e0fca14775fcb4c1cafe4dcd146654659cbade52cc12a0cdedaf83f67db7\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 20.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sx87sgK7_pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa4f6d9-c8ba-444f-f75d-0dde2ec0aab5"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-zpwuwkph\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-zpwuwkph\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=c31a939bc2930064c4b4f4de5e956ce33c5d6b3dd8c2ad6759a0987db0278cf9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dxxwbym8/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTNl7BKT2Fx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mxnet.gluon import nn, rnn\n",
        "from mxnet import gluon, autograd\n",
        "import gluonnlp as nlp\n",
        "from mxnet import nd \n",
        "import mxnet as mx\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "from kobert.mxnet_kobert import get_mxnet_kobert_model\n",
        "from kobert.utils import get_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc-zco-ST2F_"
      },
      "source": [
        "### Loading KoBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89lsydguT2GG"
      },
      "source": [
        "ctx = mx.gpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI841Zb38XOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf3df25-0ab7-48f8-9ed4-888147471124"
      },
      "source": [
        "bert_base, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False, ctx=ctx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NijpWe8J8isZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68c649f-4391-40c2-9187-ecaed3c57e50"
      },
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i69AUj9gT2Gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609a24ee-b10c-477c-a8bb-b9f7ceec484d"
      },
      "source": [
        "ds = gluon.data.SimpleDataset([['나 보기가 역겨워', '김소월']])\n",
        "trans = nlp.data.BERTSentenceTransform(tok, max_seq_length=10)\n",
        "\n",
        "list(ds.transform(trans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([   2, 1370, 2362, 5330, 3322,    3, 1316, 6607, 7028,    3],\n",
              "        dtype=int32),\n",
              "  array(10, dtype=int32),\n",
              "  array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], dtype=int32))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8jwdS6XKFu"
      },
      "source": [
        "### Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTs6-MFnHT9w"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da-FuxMv40b-"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/problem_complete.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJyWHAA3hmZL"
      },
      "source": [
        "###train data, test data 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuaW0Ub2JtOJ"
      },
      "source": [
        "train_df, remaining = train_test_split(df, random_state = 52, train_size = 0.7, stratify = df.tag.values)\n",
        "valid_df, _ = train_test_split(remaining, random_state=52, train_size = 0.2, stratify = remaining.tag.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfkpxko8d9f9",
        "outputId": "092a5c89-3b2d-4441-8c88-2e070d4bf3ed"
      },
      "source": [
        "print(train_df.shape, valid_df.shape)\n",
        "\n",
        "#라벨의 숫자화\n",
        "label = df['tag'].values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3952, 7) (338, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRoAvVehv0Y"
      },
      "source": [
        "###input data로 변환 (문제, label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569PkR09LGFY"
      },
      "source": [
        "dataset_train = []\n",
        "dataset_test = []\n",
        "\n",
        "tmp = encoder.transform(train_df['tag'])\n",
        "i = 0\n",
        "for index, row in train_df.iterrows():\n",
        "  problem = row['description']\n",
        "  if not pd.isna(row['input']) :\n",
        "    problem = problem + row['input'] \n",
        "  if not pd.isna(row['output']) :\n",
        "      problem = problem + row['output'] \n",
        "  dataset_train.append([problem , tmp[i]])\n",
        "  i = i + 1\n",
        "\n",
        "i = 0\n",
        "tmp = encoder.transform(valid_df['tag'])\n",
        "for index, row in valid_df.iterrows():\n",
        "  problem = row['description']\n",
        "  if not pd.isna(row['input']) :\n",
        "    problem = problem + row['input'] \n",
        "  if not pd.isna(row['output']) :\n",
        "      problem = problem + row['output'] \n",
        "  dataset_test.append([problem , tmp[i]])\n",
        "  i = i + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORlB84Iqh8kz"
      },
      "source": [
        "###Bert에 맞게 input 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0raV8uT2G2"
      },
      "source": [
        "class BERTDataset(mx.gluon.data.Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        sent_dataset = gluon.data.SimpleDataset([[\n",
        "            i[sent_idx],\n",
        "        ] for i in dataset])\n",
        "        self.sentences = sent_dataset.transform(transform)\n",
        "        self.labels = gluon.data.SimpleDataset(\n",
        "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtk-8pQST2G9"
      },
      "source": [
        "max_len = 512 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_BLZP_T2HF"
      },
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk1MRxVLiHKR"
      },
      "source": [
        "###Bert 다음 layer 설정 (single + dropout 추가)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhaw0H4ST2HM"
      },
      "source": [
        "class BERTClassifier(nn.Block):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 num_classes=2,#change\n",
        "                 dropout=None,\n",
        "                 prefix=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
        "        self.bert = bert\n",
        "        with self.name_scope():\n",
        "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
        "            self.classifier.add(nn.Dense(256)) # add layer\n",
        "            if dropout:\n",
        "                self.classifier.add(nn.Dropout(rate=dropout))\n",
        "            self.classifier.add(nn.Dense(units=num_classes))\n",
        "\n",
        "    def forward(self, inputs, token_types, valid_length=None):\n",
        "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
        "        return self.classifier(pooler)\n",
        "                                           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y00BOPwST2HX"
      },
      "source": [
        "model = BERTClassifier(bert_base, num_classes=11, dropout=0.1)\n",
        "# 분류 레이어만 초기화 한다. \n",
        "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
        "model.hybridize()\n",
        "\n",
        "# softmax cross entropy loss for classification\n",
        "loss_function = gluon.loss.SoftmaxCELoss()\n",
        "\n",
        "metric = mx.metric.Accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2dLhnHkT2Hf"
      },
      "source": [
        "batch_size = 8\n",
        "lr = 5e-5\n",
        "\n",
        "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=int(batch_size/2), num_workers=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESo76UH-T2Hr"
      },
      "source": [
        "trainer = gluon.Trainer(model.collect_params(), 'bertadam',\n",
        "                        {'learning_rate': lr, 'epsilon': 1e-9, 'wd':0.01})\n",
        "\n",
        "log_interval = 4\n",
        "num_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wspMBDOAT2H0"
      },
      "source": [
        "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
        "for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n",
        "    v.wd_mult = 0.0\n",
        "params = [\n",
        "    p for p in model.collect_params().values() if p.grad_req != 'null'\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCR6AMKHT2H6"
      },
      "source": [
        "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
        "    acc = mx.metric.Accuracy()\n",
        "    i = 0\n",
        "    for i, (t,v,s, label) in enumerate(data_iter):\n",
        "        token_ids = t.as_in_context(ctx)\n",
        "        valid_length = v.as_in_context(ctx)\n",
        "        segment_ids = s.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
        "        acc.update(preds=output, labels=label)\n",
        "        if i > 1000:\n",
        "            break\n",
        "        i += 1\n",
        "    return(acc.get()[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkcW6GyeT2IA"
      },
      "source": [
        "#learning rate warmup을 위한 준비 \n",
        "accumulate = 4\n",
        "step_size = batch_size * accumulate if accumulate else batch_size\n",
        "num_train_examples = len(data_train)\n",
        "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
        "warmup_ratio = 0.1\n",
        "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
        "step_num = 0\n",
        "all_model_params = model.collect_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf_rpZTq6uES"
      },
      "source": [
        "# Set grad_req if gradient accumulation is required\n",
        "if accumulate and accumulate > 1:\n",
        "    for p in params:\n",
        "        p.grad_req = 'add'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mJ3Pw_VT2IH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb70c7ba-4bec-4767-9181-925188f17a4e"
      },
      "source": [
        "for epoch_id in range(num_epochs):\n",
        "    metric.reset()\n",
        "    step_loss = 0\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
        "        if step_num < num_warmup_steps:\n",
        "            new_lr = lr * step_num / num_warmup_steps\n",
        "        else:\n",
        "            non_warmup_steps = step_num - num_warmup_steps\n",
        "            offset = non_warmup_steps / (num_train_steps - num_warmup_steps)\n",
        "            new_lr = lr - offset * lr\n",
        "        trainer.set_learning_rate(new_lr)\n",
        "        with mx.autograd.record():\n",
        "            # load data to GPU\n",
        "            token_ids = token_ids.as_in_context(ctx)\n",
        "            valid_length = valid_length.as_in_context(ctx)\n",
        "            segment_ids = segment_ids.as_in_context(ctx)\n",
        "            label = label.as_in_context(ctx)\n",
        "\n",
        "            # forward computation\n",
        "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
        "            ls = loss_function(out, label).mean()\n",
        "\n",
        "        # backward computation\n",
        "        ls.backward()\n",
        "        if not accumulate or (batch_id + 1) % accumulate == 0:\n",
        "          trainer.allreduce_grads()\n",
        "          nlp.utils.clip_grad_global_norm(params, 1)\n",
        "          trainer.update(accumulate if accumulate else 1)\n",
        "          step_num += 1\n",
        "          if accumulate and accumulate > 1:\n",
        "              # set grad to zero for gradient accumulation\n",
        "              all_model_params.zero_grad()\n",
        "\n",
        "        step_loss += ls.asscalar()\n",
        "        metric.update([label], [out])\n",
        "        if (batch_id + 1) % (50) == 0:\n",
        "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
        "                         .format(epoch_id + 1, batch_id + 1, len(train_dataloader),\n",
        "                                 step_loss / log_interval,\n",
        "                                 trainer.learning_rate, metric.get()[1]))\n",
        "            step_loss = 0\n",
        "    test_acc = evaluate_accuracy(model, test_dataloader, ctx)\n",
        "    print('Test Acc : {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1 Batch 50/494] loss=29.5242, lr=0.0000098361, acc=0.105\n",
            "[Epoch 1 Batch 100/494] loss=27.0090, lr=0.0000196721, acc=0.171\n",
            "[Epoch 1 Batch 150/494] loss=26.5874, lr=0.0000303279, acc=0.193\n",
            "[Epoch 1 Batch 200/494] loss=25.8800, lr=0.0000401639, acc=0.206\n",
            "[Epoch 1 Batch 250/494] loss=26.3430, lr=0.0000499101, acc=0.211\n",
            "[Epoch 1 Batch 300/494] loss=27.1371, lr=0.0000488309, acc=0.214\n",
            "[Epoch 1 Batch 350/494] loss=27.0891, lr=0.0000476619, acc=0.216\n",
            "[Epoch 1 Batch 400/494] loss=26.8233, lr=0.0000465827, acc=0.221\n",
            "[Epoch 1 Batch 450/494] loss=26.3353, lr=0.0000454137, acc=0.221\n",
            "Test Acc : 0.026627218934911243\n",
            "[Epoch 2 Batch 50/494] loss=25.7168, lr=0.0000433453, acc=0.275\n",
            "[Epoch 2 Batch 100/494] loss=26.0570, lr=0.0000422662, acc=0.256\n",
            "[Epoch 2 Batch 150/494] loss=26.6398, lr=0.0000410971, acc=0.247\n",
            "[Epoch 2 Batch 200/494] loss=25.8749, lr=0.0000400180, acc=0.242\n",
            "[Epoch 2 Batch 250/494] loss=26.2266, lr=0.0000388489, acc=0.241\n",
            "[Epoch 2 Batch 300/494] loss=27.0330, lr=0.0000377698, acc=0.240\n",
            "[Epoch 2 Batch 350/494] loss=26.8550, lr=0.0000366007, acc=0.238\n",
            "[Epoch 2 Batch 400/494] loss=26.8038, lr=0.0000355216, acc=0.241\n",
            "[Epoch 2 Batch 450/494] loss=25.8518, lr=0.0000343525, acc=0.241\n",
            "Test Acc : 0.2455621301775148\n",
            "[Epoch 3 Batch 50/494] loss=25.5218, lr=0.0000322842, acc=0.268\n",
            "[Epoch 3 Batch 100/494] loss=25.9699, lr=0.0000312050, acc=0.253\n",
            "[Epoch 3 Batch 150/494] loss=26.5141, lr=0.0000300360, acc=0.247\n",
            "[Epoch 3 Batch 200/494] loss=25.7610, lr=0.0000289568, acc=0.244\n",
            "[Epoch 3 Batch 250/494] loss=26.2140, lr=0.0000277878, acc=0.241\n",
            "[Epoch 3 Batch 300/494] loss=27.0448, lr=0.0000267086, acc=0.239\n",
            "[Epoch 3 Batch 350/494] loss=26.8016, lr=0.0000255396, acc=0.237\n",
            "[Epoch 3 Batch 400/494] loss=26.7620, lr=0.0000244604, acc=0.240\n",
            "[Epoch 3 Batch 450/494] loss=25.9063, lr=0.0000232914, acc=0.241\n",
            "Test Acc : 0.2455621301775148\n",
            "[Epoch 4 Batch 50/494] loss=25.5751, lr=0.0000212230, acc=0.268\n",
            "[Epoch 4 Batch 100/494] loss=25.9496, lr=0.0000201439, acc=0.253\n",
            "[Epoch 4 Batch 150/494] loss=26.4859, lr=0.0000189748, acc=0.247\n",
            "[Epoch 4 Batch 200/494] loss=25.7629, lr=0.0000178957, acc=0.246\n",
            "[Epoch 4 Batch 250/494] loss=26.2216, lr=0.0000167266, acc=0.242\n",
            "[Epoch 4 Batch 300/494] loss=26.9828, lr=0.0000156475, acc=0.242\n",
            "[Epoch 4 Batch 350/494] loss=26.7825, lr=0.0000144784, acc=0.240\n",
            "[Epoch 4 Batch 400/494] loss=26.7784, lr=0.0000133993, acc=0.242\n",
            "[Epoch 4 Batch 450/494] loss=25.8833, lr=0.0000122302, acc=0.242\n",
            "Test Acc : 0.2455621301775148\n",
            "[Epoch 5 Batch 50/494] loss=25.5299, lr=0.0000101619, acc=0.268\n",
            "[Epoch 5 Batch 100/494] loss=25.8655, lr=0.0000090827, acc=0.253\n",
            "[Epoch 5 Batch 150/494] loss=26.5307, lr=0.0000079137, acc=0.247\n",
            "[Epoch 5 Batch 200/494] loss=25.7616, lr=0.0000068345, acc=0.245\n",
            "[Epoch 5 Batch 250/494] loss=26.1403, lr=0.0000056655, acc=0.242\n",
            "[Epoch 5 Batch 300/494] loss=26.9514, lr=0.0000045863, acc=0.242\n",
            "[Epoch 5 Batch 350/494] loss=26.8232, lr=0.0000034173, acc=0.240\n",
            "[Epoch 5 Batch 400/494] loss=26.7605, lr=0.0000023381, acc=0.242\n",
            "[Epoch 5 Batch 450/494] loss=25.7665, lr=0.0000011691, acc=0.242\n",
            "Test Acc : 0.2455621301775148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDNfwIWj10_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}